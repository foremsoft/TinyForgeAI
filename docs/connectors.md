# TinyForgeAI Connectors Documentation
<!-- Generated by Claude Code: Step I.1 -->

**Last updated:** 2025-01-15T00:00:00Z

This document describes the data source connectors available in TinyForgeAI for loading training data from various sources.

## Overview

The Data Connector Layer provides a unified interface for ingesting training data from multiple sources. Each connector follows a consistent pattern:

- **Programmatic API**: Python classes and functions for integration into custom pipelines
- **CLI Support**: Command-line tools for quick data extraction and testing
- **Mock Mode**: Offline development support via environment variables

### Supported Data Sources

| Source | Connector | Status | Mock Mode |
|--------|-----------|--------|-----------|
| SQL Databases | `DBConnector` | Implemented | N/A (use SQLite) |
| Google Docs | `GoogleDocsConnector` | Implemented | `GOOGLE_DOCS_MOCK=true` |
| Local Files | `file_ingest` | Implemented | N/A |
| REST APIs | `APIConnector` | Implemented | `API_MOCK=true` |
| Google Drive | `GoogleDriveConnector` | Implemented | `GOOGLE_DRIVE_MOCK=true` |
| Notion | `NotionConnector` | Implemented | `NOTION_MOCK=true` |
| Slack | `SlackConnector` | Implemented | `SLACK_MOCK=true` |
| Confluence | `ConfluenceConnector` | Implemented | `CONFLUENCE_MOCK=true` |

### Architecture

```
┌─────────────────────────────────────────────────────────────┐
│                     Data Sources                            │
│  ┌─────────┐  ┌─────────────┐  ┌───────────┐  ┌──────────┐ │
│  │ SQL DB  │  │ Google Docs │  │ Files     │  │ API/etc  │ │
│  └────┬────┘  └──────┬──────┘  └─────┬─────┘  └────┬─────┘ │
└───────┼──────────────┼───────────────┼─────────────┼───────┘
        │              │               │             │
        ▼              ▼               ▼             ▼
┌─────────────────────────────────────────────────────────────┐
│                   Connector Layer                           │
│  ┌─────────────┐ ┌───────────────────┐ ┌─────────────────┐ │
│  │DBConnector  │ │google_docs_connect│ │file_ingest      │ │
│  │             │ │                   │ │                 │ │
│  │stream_sample│ │fetch_doc_text()   │ │ingest_file()    │ │
│  └──────┬──────┘ └─────────┬─────────┘ └────────┬────────┘ │
└─────────┼──────────────────┼────────────────────┼──────────┘
          │                  │                    │
          ▼                  ▼                    ▼
┌─────────────────────────────────────────────────────────────┐
│                  Training Samples (JSONL)                   │
│       {"input": "...", "output": "...", "metadata": {...}}  │
└─────────────────────────────────────────────────────────────┘
```

## Database Connector

The database connector streams training samples directly from SQL databases. It uses Python's built-in `sqlite3` module for SQLite databases.

### Quick Start

```python
from connectors.db_connector import DBConnector

# Create connector (uses DB_URL from environment by default)
conn = DBConnector(db_url="sqlite:///./data/qa.db")

# Test connection
if conn.test_connection():
    print("Connected successfully!")

# Stream training samples
query = "SELECT question, answer FROM qa_pairs"
mapping = {"input": "question", "output": "answer"}

for sample in conn.stream_samples(query, mapping):
    print(sample)
```

### Configuration

Database connection settings are configured via environment variables or the `DBSettings` class:

```bash
# SQLite file database
export DB_URL="sqlite:///./data/training.db"

# In-memory SQLite (default)
export DB_URL="sqlite:///:memory:"
```

Or in your `.env` file:

```env
DB_URL=sqlite:///./data/training.db
```

### CLI Usage

```bash
# Stream samples as JSONL
python connectors/db_connector.py \
    --query "SELECT question AS q, answer AS a FROM qa" \
    --mapping '{"input":"q","output":"a"}'

# Limit output
python connectors/db_connector.py \
    --query "SELECT question, answer FROM qa" \
    --mapping '{"input":"question","output":"answer"}' \
    --limit 10

# Specify database URL
python connectors/db_connector.py \
    --db-url "sqlite:///./data/qa.db" \
    --query "SELECT question, answer FROM qa" \
    --mapping '{"input":"question","output":"answer"}'
```

Using the connector CLI:

```bash
# Test connection
python connectors/cli.py test-connection --db-url "sqlite:///./data/qa.db"

# Stream samples
python connectors/cli.py db-stream \
    --query "SELECT question, answer FROM qa" \
    --mapping '{"input":"question","output":"answer"}'
```

### Column Mapping

The `mapping` parameter specifies which database columns correspond to training sample fields:

```python
# Direct column mapping
mapping = {"input": "question", "output": "answer"}

# With SQL aliases
query = "SELECT q AS question, a AS answer FROM faq"
mapping = {"input": "question", "output": "answer"}
```

### Output Format

Each sample includes metadata about its source:

```json
{
    "input": "How do I reset my password?",
    "output": "Go to Settings > Reset Password.",
    "metadata": {
        "source": "db",
        "raw_row": {
            "question": "How do I reset my password?",
            "answer": "Go to Settings > Reset Password."
        }
    }
}
```

### Row Mapping Utility

The `row_to_sample` function converts individual database rows to training samples:

```python
from connectors.mappers import row_to_sample

row = {"question": "What is 2+2?", "answer": "4", "category": "math"}
mapping = {"input": "question", "output": "answer"}

sample = row_to_sample(row, mapping)
```

### Error Handling

```python
from connectors.db_connector import DBConnector
from connectors.mappers import row_to_sample

# Missing column in mapping
try:
    row_to_sample(row, {"input": "nonexistent"})
except KeyError as e:
    print(f"Mapping error: {e}")

# Database connection issues
conn = DBConnector(db_url="sqlite:///nonexistent.db")
if not conn.test_connection():
    print("Connection failed")
```

### Local Development with SQLite

Create a test database for development:

```python
import sqlite3

conn = sqlite3.connect("./data/test.db")
cursor = conn.cursor()
cursor.execute("""
    CREATE TABLE qa (
        question TEXT,
        answer TEXT
    )
""")
cursor.execute("INSERT INTO qa VALUES (?, ?)",
               ("How do I reset my password?", "Go to Settings > Reset Password."))
conn.commit()
conn.close()
```

Then stream from it:

```bash
export DB_URL="sqlite:///./data/test.db"
python connectors/db_connector.py \
    --query "SELECT question, answer FROM qa" \
    --mapping '{"input":"question","output":"answer"}'
```

## Google Docs Connector

The Google Docs connector fetches and extracts text content from Google Docs using the official Google Docs API. It supports both **Service Account** and **OAuth** authentication, and includes a **mock mode** for offline development and testing.

### Quick Start

```python
from connectors.google_docs_connector import GoogleDocsConnector, GoogleDocsConfig

# Create connector (uses mock mode by default)
connector = GoogleDocsConnector()

# Fetch document text
text = connector.fetch_doc_text("your-doc-id")
print(text)

# List documents in a folder
docs = connector.list_docs_in_folder("folder-id")
for doc in docs:
    print(f"{doc.id}: {doc.title}")

# Stream training samples from a folder
for sample in connector.stream_samples("folder-id", chunk_by="paragraph"):
    print(sample)
```

### Legacy Function Interface

For backward compatibility, the connector also provides function-based access:

```python
from connectors.google_docs_connector import fetch_doc_text, list_docs_in_folder

# Fetch document text (uses mock mode by default)
text = fetch_doc_text("sample_doc1")

# List available documents
docs = list_docs_in_folder("any_folder_id")
```

### Configuration

```python
from connectors.google_docs_connector import GoogleDocsConfig

config = GoogleDocsConfig(
    # Authentication - choose one:
    service_account_file="/path/to/service-account.json",  # For service accounts
    # Or OAuth token file
    token_file="/path/to/token.json",
    credentials_file="/path/to/credentials.json",

    # Mock mode (default: True)
    mock_mode=False,
    samples_dir="./my_samples/",  # Custom mock samples directory
)

connector = GoogleDocsConnector(config)
```

### Mock Mode (Default)

By default, the connector runs in mock mode (`GOOGLE_DOCS_MOCK=true`). In this mode, it reads from local sample files in `examples/google_docs_samples/`.

```bash
# Enable mock mode (default)
export GOOGLE_DOCS_MOCK=true

# Disable mock mode (use real Google Docs API)
export GOOGLE_DOCS_MOCK=false
```

Mock mode sample files:
- `sample_doc1.txt` - Sample documentation content
- `sample_doc2.txt` - Sample FAQ content

### CLI Usage

```bash
# Fetch a document (mock mode)
python connectors/google_docs_connector.py --doc-id sample_doc1

# List documents in a folder
python connectors/google_docs_connector.py --list --folder-id my_folder

# Stream training samples
python connectors/google_docs_connector.py --stream --folder-id my_folder \
    --chunk-by paragraph --limit 100
```

### Setting Up Google Docs API (Real Mode)

#### Option 1: Service Account (Recommended for Server Applications)

1. **Create a Google Cloud Project** at [Google Cloud Console](https://console.cloud.google.com/)
2. **Enable APIs**: Google Docs API and Google Drive API
3. **Create a Service Account**:
   - Go to "IAM & Admin" > "Service Accounts"
   - Click "Create Service Account"
   - Download the JSON key file
4. **Share documents** with the service account email
5. **Configure the connector**:

```bash
export GOOGLE_DOCS_MOCK=false
```

```python
config = GoogleDocsConfig(
    mock_mode=False,
    service_account_file="./credentials/service-account.json",
)
connector = GoogleDocsConnector(config)
```

#### Option 2: OAuth (For Desktop Applications)

1. **Create OAuth Credentials**:
   - Go to "APIs & Services" > "Credentials"
   - Click "Create Credentials" > "OAuth client ID"
   - Select "Desktop application"
   - Download the credentials JSON file
2. **Install dependencies**:

```bash
pip install tinyforgeai[google]
# Or manually:
pip install google-api-python-client google-auth-httplib2 google-auth-oauthlib
```

3. **Configure and run**:

```python
config = GoogleDocsConfig(
    mock_mode=False,
    credentials_file="./credentials/oauth-credentials.json",
    token_file="./credentials/token.json",  # Will be created on first auth
)
connector = GoogleDocsConnector(config)
```

### Streaming Training Samples

The connector can stream training samples from all documents in a Google Drive folder:

```python
# Stream by paragraph (default)
for sample in connector.stream_samples("folder-id", chunk_by="paragraph"):
    print(sample)
    # {"text": "...", "metadata": {"doc_id": "...", "doc_title": "...", "chunk_index": 0}}

# Stream by section (split on headings)
for sample in connector.stream_samples("folder-id", chunk_by="section"):
    print(sample)

# Stream entire documents
for sample in connector.stream_samples("folder-id", chunk_by="document"):
    print(sample)
```

### Text Extraction

The connector extracts text from Google Docs structure including:
- Paragraphs
- Headings (all levels)
- Bulleted and numbered lists
- Tables (row by row)
- Inline objects

```python
from connectors.google_docs_connector import normalize_text

# Normalize whitespace in extracted text
messy = "Too   many    spaces\n\n\n\nand lines"
clean = normalize_text(messy)  # "Too many spaces\n\nand lines"
```

### Error Handling

```python
from connectors.google_docs_connector import GoogleDocsConnector

connector = GoogleDocsConnector()

try:
    text = connector.fetch_doc_text("nonexistent_doc")
except FileNotFoundError as e:
    print(f"Document not found: {e}")
except RuntimeError as e:
    print(f"API error: {e}")
```

### Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `GOOGLE_DOCS_MOCK` | `true` | Enable mock mode |
| `GOOGLE_SERVICE_ACCOUNT_FILE` | - | Path to service account JSON |
| `GOOGLE_CREDENTIALS_FILE` | - | Path to OAuth credentials JSON |
| `GOOGLE_TOKEN_FILE` | - | Path to OAuth token file |

## File Ingestion Connector

The file ingestion connector extracts text content from various document formats.

### Quick Start

```python
from connectors.file_ingest import ingest_file

# Ingest any supported file
text = ingest_file("path/to/document.txt")
text = ingest_file("path/to/document.md")
text = ingest_file("path/to/document.docx")
text = ingest_file("path/to/document.pdf")
```

### Supported Formats

| Format | Extension | Dependency | Notes |
|--------|-----------|------------|-------|
| Plain Text | `.txt` | None (built-in) | UTF-8 encoding by default |
| Markdown | `.md` | None (built-in) | Returns raw markdown (no rendering) |
| Word Document | `.docx` | python-docx | Extracts paragraph text |
| PDF | `.pdf` | PyMuPDF or pdfminer.six | PyMuPDF preferred for speed |

### Installation

TXT and MD files work out of the box. For DOCX and PDF support, install optional dependencies:

```bash
# For DOCX support
pip install python-docx

# For PDF support (choose one)
pip install PyMuPDF        # Recommended: faster, more accurate
pip install pdfminer.six   # Alternative: pure Python
```

### Checking Available Formats

```python
from connectors.file_ingest import get_supported_formats, check_dependencies

# Check what formats are available
formats = get_supported_formats()
print(formats[".pdf"]["available"])  # True if PDF lib installed

# Check which optional dependencies are installed
deps = check_dependencies()
print(deps)
# {'python-docx': True, 'PyMuPDF': True, 'pdfminer.six': False}
```

### Custom Encoding

For TXT and MD files, you can specify a custom encoding:

```python
# Read a file with Latin-1 encoding
text = ingest_file("legacy_doc.txt", encoding="latin-1")
```

### Error Handling

```python
from connectors.file_ingest import ingest_file

try:
    text = ingest_file("document.pdf")
except FileNotFoundError as e:
    print(f"File not found: {e}")
except ValueError as e:
    print(f"Unsupported format: {e}")
except RuntimeError as e:
    print(f"Missing dependency: {e}")
```

### Sample Files

Sample files for testing are available in `examples/files/`:
- `sample.txt` - Plain text sample
- `sample.md` - Markdown sample with headings and formatting
- `sample.docx` - Word document sample (if python-docx installed)
- `sample.pdf` - PDF document sample (if PDF library installed)

## REST API Connector

The REST API connector fetches and transforms data from external REST APIs into training samples. It supports pagination, authentication, rate limiting, caching, and retry logic.

### Quick Start

```python
from connectors.api_connector import APIConnector, APIConnectorConfig

# Create connector with configuration
config = APIConnectorConfig(
    base_url="https://api.example.com",
    auth_type="bearer",
    auth_token="your-api-token",
)

connector = APIConnector(config)

# Fetch a single endpoint
data = connector.fetch("/users/1")

# Stream training samples with field mapping
mapping = {"input": "question", "output": "answer"}
for sample in connector.stream_samples("/faq", mapping):
    print(sample)
```

### Configuration

```python
from connectors.api_connector import APIConnectorConfig

config = APIConnectorConfig(
    # Base URL for all requests
    base_url="https://api.example.com",

    # Authentication (none, basic, bearer, api_key)
    auth_type="bearer",
    auth_token="your-token",

    # Or for API key authentication
    # auth_type="api_key",
    # api_key_header="X-API-Key",
    # api_key="your-key",

    # Rate limiting (requests per minute)
    rate_limit=60,

    # Retry configuration
    retry_attempts=3,
    retry_delay=1.0,  # seconds

    # Request timeout
    timeout=30,

    # Response caching
    cache_enabled=True,
    cache_ttl=300,  # seconds
)
```

### Pagination Support

The connector supports multiple pagination strategies:

```python
from connectors.api_connector import PaginationConfig, PaginationType

# Offset-based pagination
pagination = PaginationConfig(
    type=PaginationType.OFFSET,
    page_param="offset",
    limit_param="limit",
    limit=100,
)

# Page number pagination
pagination = PaginationConfig(
    type=PaginationType.PAGE,
    page_param="page",
    limit_param="per_page",
    limit=50,
)

# Cursor-based pagination
pagination = PaginationConfig(
    type=PaginationType.CURSOR,
    cursor_param="cursor",
    cursor_path="meta.next_cursor",
)

# Link header pagination (RFC 5988)
pagination = PaginationConfig(
    type=PaginationType.LINK_HEADER,
)

# Stream with pagination
for sample in connector.stream_paginated("/items", mapping, pagination):
    print(sample)
```

### Authentication Types

```python
# No authentication
config = APIConnectorConfig(base_url="https://api.example.com")

# Bearer token
config = APIConnectorConfig(
    base_url="https://api.example.com",
    auth_type="bearer",
    auth_token="your-jwt-token",
)

# Basic auth
config = APIConnectorConfig(
    base_url="https://api.example.com",
    auth_type="basic",
    auth_username="user",
    auth_password="pass",
)

# API key in header
config = APIConnectorConfig(
    base_url="https://api.example.com",
    auth_type="api_key",
    api_key_header="X-API-Key",
    api_key="your-api-key",
)
```

### Field Mapping

Map API response fields to training sample format:

```python
# Simple mapping
mapping = {"input": "question", "output": "answer"}

# Nested field access with dot notation
mapping = {"input": "data.title", "output": "data.content"}

# Transform to standard format
sample = connector.fetch_as_sample("/item/1", mapping)
# Returns: {"input": "...", "output": "...", "metadata": {...}}
```

### CLI Usage

```bash
# Fetch and display as JSON
python -m connectors.api_connector \
    --base-url "https://api.example.com" \
    --endpoint "/items" \
    --mapping '{"input": "title", "output": "body"}'

# With authentication
python -m connectors.api_connector \
    --base-url "https://api.example.com" \
    --endpoint "/items" \
    --auth-type bearer \
    --auth-token "your-token" \
    --mapping '{"input": "title", "output": "body"}'

# With pagination
python -m connectors.api_connector \
    --base-url "https://api.example.com" \
    --endpoint "/items" \
    --paginate \
    --limit 100 \
    --mapping '{"input": "title", "output": "body"}'
```

### Error Handling

```python
from connectors.api_connector import (
    APIConnector,
    APIConnectorError,
    RateLimitError,
    AuthenticationError,
)

connector = APIConnector(config)

try:
    data = connector.fetch("/endpoint")
except RateLimitError as e:
    print(f"Rate limited. Retry after {e.retry_after} seconds")
except AuthenticationError as e:
    print(f"Auth failed: {e}")
except APIConnectorError as e:
    print(f"API error: {e.status_code} - {e.message}")
```

### Caching

```python
config = APIConnectorConfig(
    base_url="https://api.example.com",
    cache_enabled=True,
    cache_ttl=300,  # Cache responses for 5 minutes
)

connector = APIConnector(config)

# First call fetches from API
data = connector.fetch("/expensive-endpoint")

# Second call returns cached response
data = connector.fetch("/expensive-endpoint")

# Force refresh
data = connector.fetch("/expensive-endpoint", force_refresh=True)

# Clear all cache
connector.clear_cache()
```

### Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `API_BASE_URL` | - | Default base URL |
| `API_AUTH_TOKEN` | - | Default bearer token |
| `API_RATE_LIMIT` | `60` | Requests per minute |
| `API_TIMEOUT` | `30` | Request timeout in seconds |
| `API_MOCK` | `false` | Enable mock mode for testing |

---

## Google Drive Connector

The Google Drive connector accesses files and folders from Google Drive. It includes a **mock mode** for offline development that reads from local sample files.

### Quick Start

```python
from connectors.google_drive_connector import GoogleDriveConnector, GoogleDriveConfig

# Create connector (uses mock mode by default)
connector = GoogleDriveConnector()

# List files in a folder
files = connector.list_files(folder_id="my_folder")
for f in files:
    print(f"{f.id}: {f.name}")

# Download file content
content = connector.get_file_content(file_id="my_file")

# Stream training samples
mapping = {"input": "input", "output": "output"}
for sample in connector.stream_samples("folder_id", mapping):
    print(sample)
```

### Configuration

```python
from connectors.google_drive_connector import GoogleDriveConfig

config = GoogleDriveConfig(
    # Authentication
    service_account_file="/path/to/service-account.json",  # For service accounts
    # Or OAuth token file
    token_file="/path/to/token.json",

    # Mock mode (default: True)
    mock_mode=False,
    samples_dir="./my_samples/",  # Custom mock samples directory

    # Query settings
    page_size=100,
    include_trashed=False,
)

connector = GoogleDriveConnector(config)
```

### Mock Mode (Default)

By default, the connector runs in mock mode (`GOOGLE_DRIVE_MOCK=true`). In this mode, it reads from local sample files in `examples/google_drive_samples/`.

```bash
# Enable mock mode (default)
export GOOGLE_DRIVE_MOCK=true

# Disable mock mode (use real Google Drive API)
export GOOGLE_DRIVE_MOCK=false
```

### CLI Usage

```bash
# List files (mock mode)
python connectors/google_drive_connector.py --list

# List files in a folder
python connectors/google_drive_connector.py --list --folder-id my_folder

# Download a file
python connectors/google_drive_connector.py --file-id sample_doc

# Stream training samples
python connectors/google_drive_connector.py --stream --folder-id my_folder \
    --mapping '{"input": "input", "output": "output"}'
```

### Setting Up Google Drive API (Real Mode)

1. **Create a Google Cloud Project** and enable the Google Drive API
2. **Create a Service Account** or OAuth credentials
3. **Download credentials** JSON file
4. **Configure the connector**:

```bash
export GOOGLE_DRIVE_MOCK=false
```

```python
config = GoogleDriveConfig(
    mock_mode=False,
    service_account_file="./credentials/service-account.json",
)
connector = GoogleDriveConnector(config)
```

### Supported File Types

The connector automatically handles Google Workspace files:

| Google Type | Export Format |
|-------------|---------------|
| Google Docs | text/plain |
| Google Sheets | text/csv |
| Google Slides | text/plain |

Native files (.txt, .json, .jsonl, .csv, .md) are downloaded as-is.

---

## Notion Connector

The Notion connector accesses pages and databases from Notion workspaces. It includes a **mock mode** for offline development.

### Quick Start

```python
from connectors.notion_connector import NotionConnector, NotionConfig

# Create connector (uses mock mode by default)
connector = NotionConnector()

# List pages in a database
pages = connector.list_pages(database_id="my_database")
for p in pages:
    print(f"{p.id}: {p.title}")

# Get page content
content = connector.get_page_content(page_id="page_id")

# Stream training samples from database
mapping = {"input": "Question", "output": "Answer"}
for sample in connector.stream_samples("database_id", mapping):
    print(sample)
```

### Configuration

```python
from connectors.notion_connector import NotionConfig

config = NotionConfig(
    # Authentication
    api_token="secret_xxx...",  # Notion integration token

    # Mock mode (default: True)
    mock_mode=False,
    samples_dir="./my_samples/",  # Custom mock samples directory

    # Query settings
    page_size=100,
    include_children=True,
    max_depth=3,
)

connector = NotionConnector(config)
```

### Mock Mode (Default)

By default, the connector runs in mock mode (`NOTION_MOCK=true`). In this mode, it reads from local sample files in `examples/notion_samples/`.

```bash
# Enable mock mode (default)
export NOTION_MOCK=true

# Disable mock mode (use real Notion API)
export NOTION_MOCK=false
export NOTION_API_TOKEN="secret_xxx..."
```

### CLI Usage

```bash
# List pages in a database (mock mode)
python connectors/notion_connector.py --list --database-id training_database

# Get page content
python connectors/notion_connector.py --page-id page-001 --content

# Stream training samples
python connectors/notion_connector.py --stream --database-id training_database \
    --mapping '{"input": "Question", "output": "Answer"}'
```

### Setting Up Notion Integration (Real Mode)

1. **Create a Notion Integration** at https://www.notion.so/my-integrations
2. **Copy the integration token** (starts with `secret_`)
3. **Share databases/pages** with your integration
4. **Configure the connector**:

```bash
export NOTION_MOCK=false
export NOTION_API_TOKEN="secret_xxx..."
```

```python
config = NotionConfig(
    mock_mode=False,
    api_token="secret_xxx...",
)
connector = NotionConnector(config)
```

### Property Types Supported

The connector extracts values from these Notion property types:

| Property Type | Extraction |
|--------------|------------|
| title | Plain text |
| rich_text | Plain text |
| number | String conversion |
| select | Selection name |
| multi_select | Comma-separated names |
| checkbox | "True"/"False" |
| url | URL string |
| email | Email string |
| date | Start date |

### Block Types Supported

When extracting page content, these block types are parsed:

- Paragraphs
- Headings (H1, H2, H3) - prefixed with #, ##, ###
- Bulleted lists - prefixed with -
- Numbered lists - prefixed with 1.
- Quotes - prefixed with >
- Code blocks - wrapped in ```

---

## Slack Connector

The Slack connector accesses messages and conversations from Slack workspaces. It includes a **mock mode** for offline development and supports multiple streaming modes for extracting training samples.

### Quick Start

```python
from connectors.slack_connector import SlackConnector, SlackConfig

# Create connector (uses mock mode by default)
connector = SlackConnector()

# List channels
channels = connector.list_channels()
for ch in channels:
    print(f"{ch.id}: {ch.name}")

# Get messages from a channel
messages = connector.get_messages("general", limit=50)
for msg in messages:
    print(f"{msg.user_id}: {msg.text}")

# Get thread replies
replies = connector.get_thread_replies("general", "1234567890.123456")

# Stream training samples
mapping = {"input": "question", "output": "answer"}
for sample in connector.stream_samples("general", mapping, mode="thread_qa"):
    print(sample)
```

### Configuration

```python
from connectors.slack_connector import SlackConfig

config = SlackConfig(
    # Authentication
    bot_token="xoxb-xxx...",  # Slack bot token

    # Mock mode (default: True)
    mock_mode=False,
    samples_dir="./my_samples/",  # Custom mock samples directory

    # Query settings
    default_limit=100,
    include_archived=False,
)

connector = SlackConnector(config)
```

### Mock Mode (Default)

By default, the connector runs in mock mode (`SLACK_MOCK=true`). In this mode, it reads from local sample files in `examples/slack_samples/`.

```bash
# Enable mock mode (default)
export SLACK_MOCK=true

# Disable mock mode (use real Slack API)
export SLACK_MOCK=false
export SLACK_BOT_TOKEN="xoxb-xxx..."
```

### Streaming Modes

The connector supports multiple streaming modes for extracting training samples:

#### Thread Q&A Mode (Default)
Extracts question/answer pairs from threaded conversations. The first message in a thread is treated as the question, and replies are treated as answers.

```python
for sample in connector.stream_samples("general", mapping, mode="thread_qa"):
    print(sample)
# Returns: {"input": "How do I...?", "output": "You can...", "metadata": {...}}
```

#### Consecutive Mode
Creates pairs from consecutive messages in a channel.

```python
for sample in connector.stream_samples("general", mapping, mode="consecutive"):
    print(sample)
```

#### Reaction Filter Mode
Extracts messages that have specific reactions (e.g., messages marked with thumbsup).

```python
for sample in connector.stream_samples(
    "general",
    mapping,
    mode="reaction_filter",
    reaction_filter=["thumbsup", "white_check_mark"]
):
    print(sample)
```

### Message Text Cleaning

The connector automatically cleans message text by:
- Removing user mentions (`<@U123>` → `@user`)
- Removing channel mentions (`<#C123>` → `#channel`)
- Removing URLs (`<https://...>` → `https://...`)
- Removing special Slack commands

```python
# Manual text cleaning
clean_text = connector._clean_message_text(raw_text)
```

### CLI Usage

```bash
# List channels (mock mode)
python connectors/slack_connector.py --list

# Get messages from a channel
python connectors/slack_connector.py --channel general --limit 50

# Get thread replies
python connectors/slack_connector.py --channel general --thread 1234567890.123456

# Stream training samples
python connectors/slack_connector.py --stream --channel general \
    --mapping '{"input": "question", "output": "answer"}' \
    --mode thread_qa
```

### Setting Up Slack App (Real Mode)

1. **Create a Slack App** at https://api.slack.com/apps
2. **Add Bot Token Scopes**:
   - `channels:history` - View messages in public channels
   - `channels:read` - View basic channel information
   - `groups:history` - View messages in private channels (optional)
   - `groups:read` - View basic private channel information (optional)
   - `users:read` - View users in the workspace
3. **Install the app** to your workspace
4. **Copy the Bot User OAuth Token** (starts with `xoxb-`)
5. **Configure the connector**:

```bash
export SLACK_MOCK=false
export SLACK_BOT_TOKEN="xoxb-xxx..."
```

```python
config = SlackConfig(
    mock_mode=False,
    bot_token="xoxb-xxx...",
)
connector = SlackConnector(config)
```

### Async Slack Connector

For high-performance applications, use the async version:

```python
from connectors.async_slack_connector import AsyncSlackConnector

async with AsyncSlackConnector() as connector:
    # Concurrent message fetching
    channels = ["general", "random", "help"]
    messages = await connector.fetch_messages_concurrent(channels, max_concurrency=5)

    # Async streaming
    async for sample in connector.stream_samples("general", mapping):
        print(sample)
```

---

## Confluence Connector

The Confluence connector accesses pages and spaces from Atlassian Confluence. It includes a **mock mode** for offline development and supports HTML to text conversion.

### Quick Start

```python
from connectors.confluence_connector import ConfluenceConnector, ConfluenceConfig

# Create connector (uses mock mode by default)
connector = ConfluenceConnector()

# List spaces
spaces = connector.list_spaces()
for space in spaces:
    print(f"{space.key}: {space.name}")

# List pages in a space
pages = connector.list_pages(space_key="DOCS")
for page in pages:
    print(f"{page.id}: {page.title}")

# Get page content
content = connector.get_page_content(page_id="12345")

# Stream training samples
mapping = {"input": "title", "output": "content"}
for sample in connector.stream_samples("DOCS", mapping):
    print(sample)
```

### Configuration

```python
from connectors.confluence_connector import ConfluenceConfig

config = ConfluenceConfig(
    # Authentication
    base_url="https://your-domain.atlassian.net/wiki",
    username="your-email@example.com",
    api_token="your-api-token",

    # Mock mode (default: True)
    mock_mode=False,
    samples_dir="./my_samples/",  # Custom mock samples directory

    # Query settings
    page_size=25,
)

connector = ConfluenceConnector(config)
```

### Mock Mode (Default)

By default, the connector runs in mock mode (`CONFLUENCE_MOCK=true`). In this mode, it reads from local sample files in `examples/confluence_samples/`.

```bash
# Enable mock mode (default)
export CONFLUENCE_MOCK=true

# Disable mock mode (use real Confluence API)
export CONFLUENCE_MOCK=false
export CONFLUENCE_BASE_URL="https://your-domain.atlassian.net/wiki"
export CONFLUENCE_USERNAME="your-email@example.com"
export CONFLUENCE_API_TOKEN="your-api-token"
```

### Listing Spaces

```python
# List all spaces
spaces = connector.list_spaces()

# Filter by type
global_spaces = connector.list_spaces(type_filter="global")
personal_spaces = connector.list_spaces(type_filter="personal")

# Filter by status
active_spaces = connector.list_spaces(status="current")
```

### Searching Pages

Use CQL (Confluence Query Language) for advanced searches:

```python
# Search by text
results = connector.search("password reset", space_key="DOCS")

# Search with CQL
results = connector.search(
    query='type=page AND label="important"',
    space_key="DOCS",
    limit=50
)
```

### HTML to Text Conversion

The connector automatically converts HTML content to plain text:

```python
from connectors.confluence_connector import html_to_text

html = "<p>Hello <strong>world</strong>!</p><ul><li>Item 1</li></ul>"
text = html_to_text(html)
# Returns: "Hello world!\n\n- Item 1"
```

### CLI Usage

```bash
# List spaces (mock mode)
python connectors/confluence_connector.py --list-spaces

# List pages in a space
python connectors/confluence_connector.py --list-pages --space-key DOCS

# Get page content
python connectors/confluence_connector.py --page-id 12345 --content

# Search pages
python connectors/confluence_connector.py --search "password reset" --space-key DOCS

# Stream training samples
python connectors/confluence_connector.py --stream --space-key DOCS \
    --mapping '{"input": "title", "output": "content"}'
```

### Setting Up Confluence API (Real Mode)

1. **Generate an API token** at https://id.atlassian.com/manage-profile/security/api-tokens
2. **Note your Confluence URL** (e.g., `https://your-domain.atlassian.net/wiki`)
3. **Configure the connector**:

```bash
export CONFLUENCE_MOCK=false
export CONFLUENCE_BASE_URL="https://your-domain.atlassian.net/wiki"
export CONFLUENCE_USERNAME="your-email@example.com"
export CONFLUENCE_API_TOKEN="your-api-token"
```

```python
config = ConfluenceConfig(
    mock_mode=False,
    base_url="https://your-domain.atlassian.net/wiki",
    username="your-email@example.com",
    api_token="your-api-token",
)
connector = ConfluenceConnector(config)
```

### Content Formats

When retrieving page content, you can specify the format:

```python
# Plain text (default) - HTML is converted to text
text = connector.get_page_content(page_id="12345", format="text")

# Raw HTML storage format
html = connector.get_page_content(page_id="12345", format="storage")

# View format (rendered HTML)
html = connector.get_page_content(page_id="12345", format="view")
```

---

## Adding a New Connector

To add a new connector, follow these patterns:

### 1. Create the Connector Module

Create a new file in `connectors/`, e.g., `connectors/notion_connector.py`:

```python
"""
Notion connector for TinyForgeAI.

Provides functionality to fetch content from Notion pages and databases.
"""

import os
from typing import Iterator, Optional


def _is_mock_mode() -> bool:
    """Check if running in mock mode."""
    # Support both connector-specific and global mock env vars
    if os.getenv("NOTION_MOCK", "").lower() in ("true", "1", "yes"):
        return True
    if os.getenv("CONNECTOR_MOCK", "").lower() in ("true", "1", "yes"):
        return True
    return False


def fetch_page_content(page_id: str) -> str:
    """
    Fetch content from a Notion page.

    Args:
        page_id: The Notion page ID.

    Returns:
        The text content of the page.
    """
    if _is_mock_mode():
        return _fetch_page_mock(page_id)
    return _fetch_page_real(page_id)


def _fetch_page_mock(page_id: str) -> str:
    """Mock implementation for testing."""
    # Read from local sample files
    ...


def _fetch_page_real(page_id: str) -> str:
    """Real implementation using Notion API."""
    # Use notion-client library
    ...


def stream_samples(database_id: str, mapping: dict) -> Iterator[dict]:
    """
    Stream training samples from a Notion database.

    Args:
        database_id: The Notion database ID.
        mapping: Column mapping for input/output fields.

    Yields:
        Training sample dicts.
    """
    ...
```

### 2. Add Tests

Create `tests/test_notion_connector.py`:

```python
"""Tests for Notion connector."""

import pytest
from connectors.notion_connector import fetch_page_content


def test_fetch_page_mock_mode(monkeypatch):
    """Test fetching page in mock mode."""
    monkeypatch.setenv("NOTION_MOCK", "true")
    # Create sample file in examples/notion_samples/
    text = fetch_page_content("sample_page1")
    assert len(text) > 0


def test_fetch_page_not_found(monkeypatch):
    """Test error when page not found."""
    monkeypatch.setenv("NOTION_MOCK", "true")
    with pytest.raises(FileNotFoundError):
        fetch_page_content("nonexistent")
```

### 3. Add CLI Support

Add commands to `connectors/cli.py`:

```python
@cli.command("notion-fetch")
@click.option("--page-id", required=True, help="Notion page ID")
def notion_fetch(page_id: str):
    """Fetch content from a Notion page."""
    from connectors.notion_connector import fetch_page_content
    print(fetch_page_content(page_id))
```

### 4. Update Documentation

Add a section to this file documenting the new connector.

## Environment Variables Reference

| Variable | Default | Description |
|----------|---------|-------------|
| `DB_URL` | `sqlite:///:memory:` | Database connection URL |
| `GOOGLE_DOCS_MOCK` | `true` | Enable mock mode for Google Docs |
| `GOOGLE_SERVICE_ACCOUNT_FILE` | - | Path to Google service account JSON |
| `GOOGLE_CREDENTIALS_FILE` | - | Path to Google OAuth credentials JSON |
| `GOOGLE_TOKEN_FILE` | - | Path to Google OAuth token file |
| `GOOGLE_DRIVE_MOCK` | `true` | Enable mock mode for Google Drive |
| `NOTION_MOCK` | `true` | Enable mock mode for Notion |
| `NOTION_API_TOKEN` | - | Notion integration token |
| `SLACK_MOCK` | `true` | Enable mock mode for Slack |
| `SLACK_BOT_TOKEN` | - | Slack bot token |
| `CONFLUENCE_MOCK` | `true` | Enable mock mode for Confluence |
| `CONFLUENCE_BASE_URL` | - | Confluence instance URL |
| `CONFLUENCE_USERNAME` | - | Confluence username/email |
| `CONFLUENCE_API_TOKEN` | - | Confluence API token |
| `CONNECTOR_MOCK` | `false` | Enable mock mode for all connectors |
| `API_BASE_URL` | - | Default REST API base URL |
| `API_AUTH_TOKEN` | - | Default bearer token for API |
| `API_RATE_LIMIT` | `60` | API requests per minute |
| `API_TIMEOUT` | `30` | API request timeout in seconds |
| `API_MOCK` | `false` | Enable mock mode for API connector |

## Troubleshooting

### Database Connection Fails

```bash
# Check if database file exists
ls -la ./data/

# Test with in-memory database
python -c "from connectors.db_connector import DBConnector; print(DBConnector().test_connection())"
```

### Google Docs Mock File Not Found

```bash
# Check if sample files exist
ls -la examples/google_docs_samples/

# Verify GOOGLE_OAUTH_DISABLED is set
echo $GOOGLE_OAUTH_DISABLED
```

### PDF/DOCX Ingestion Fails

```bash
# Check which dependencies are installed
python -c "from connectors.file_ingest import check_dependencies; print(check_dependencies())"

# Install missing dependencies
pip install python-docx PyMuPDF
```

### Unicode/Encoding Errors

```python
# Specify encoding explicitly
from connectors.file_ingest import ingest_file
text = ingest_file("file.txt", encoding="utf-8-sig")  # For files with BOM
text = ingest_file("file.txt", encoding="latin-1")    # For legacy files
```

## Related Documentation

- [Training Documentation](training.md) - Using connectors with training pipeline
- [Architecture Overview](architecture.md) - System design and components
- [CI/CD](ci.md) - Continuous integration setup
