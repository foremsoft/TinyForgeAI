# TinyForgeAI Connectors Documentation
<!-- Generated by Claude Code: Step I.1 -->

**Last updated:** 2025-01-15T00:00:00Z

This document describes the data source connectors available in TinyForgeAI for loading training data from various sources.

## Overview

The Data Connector Layer provides a unified interface for ingesting training data from multiple sources. Each connector follows a consistent pattern:

- **Programmatic API**: Python classes and functions for integration into custom pipelines
- **CLI Support**: Command-line tools for quick data extraction and testing
- **Mock Mode**: Offline development support via environment variables

### Supported Data Sources

| Source | Connector | Status | Mock Mode |
|--------|-----------|--------|-----------|
| SQL Databases | `DBConnector` | Implemented | N/A (use SQLite) |
| Google Docs | `google_docs_connector` | Implemented | `GOOGLE_OAUTH_DISABLED=true` |
| Local Files | `file_ingest` | Implemented | N/A |
| REST APIs | `APIConnector` | Implemented | `API_MOCK=true` |
| Google Drive | `GoogleDriveConnector` | Implemented | `GOOGLE_DRIVE_MOCK=true` |
| Notion | `NotionConnector` | Implemented | `NOTION_MOCK=true` |

### Architecture

```
┌─────────────────────────────────────────────────────────────┐
│                     Data Sources                            │
│  ┌─────────┐  ┌─────────────┐  ┌───────────┐  ┌──────────┐ │
│  │ SQL DB  │  │ Google Docs │  │ Files     │  │ API/etc  │ │
│  └────┬────┘  └──────┬──────┘  └─────┬─────┘  └────┬─────┘ │
└───────┼──────────────┼───────────────┼─────────────┼───────┘
        │              │               │             │
        ▼              ▼               ▼             ▼
┌─────────────────────────────────────────────────────────────┐
│                   Connector Layer                           │
│  ┌─────────────┐ ┌───────────────────┐ ┌─────────────────┐ │
│  │DBConnector  │ │google_docs_connect│ │file_ingest      │ │
│  │             │ │                   │ │                 │ │
│  │stream_sample│ │fetch_doc_text()   │ │ingest_file()    │ │
│  └──────┬──────┘ └─────────┬─────────┘ └────────┬────────┘ │
└─────────┼──────────────────┼────────────────────┼──────────┘
          │                  │                    │
          ▼                  ▼                    ▼
┌─────────────────────────────────────────────────────────────┐
│                  Training Samples (JSONL)                   │
│       {"input": "...", "output": "...", "metadata": {...}}  │
└─────────────────────────────────────────────────────────────┘
```

## Database Connector

The database connector streams training samples directly from SQL databases. It uses Python's built-in `sqlite3` module for SQLite databases.

### Quick Start

```python
from connectors.db_connector import DBConnector

# Create connector (uses DB_URL from environment by default)
conn = DBConnector(db_url="sqlite:///./data/qa.db")

# Test connection
if conn.test_connection():
    print("Connected successfully!")

# Stream training samples
query = "SELECT question, answer FROM qa_pairs"
mapping = {"input": "question", "output": "answer"}

for sample in conn.stream_samples(query, mapping):
    print(sample)
```

### Configuration

Database connection settings are configured via environment variables or the `DBSettings` class:

```bash
# SQLite file database
export DB_URL="sqlite:///./data/training.db"

# In-memory SQLite (default)
export DB_URL="sqlite:///:memory:"
```

Or in your `.env` file:

```env
DB_URL=sqlite:///./data/training.db
```

### CLI Usage

```bash
# Stream samples as JSONL
python connectors/db_connector.py \
    --query "SELECT question AS q, answer AS a FROM qa" \
    --mapping '{"input":"q","output":"a"}'

# Limit output
python connectors/db_connector.py \
    --query "SELECT question, answer FROM qa" \
    --mapping '{"input":"question","output":"answer"}' \
    --limit 10

# Specify database URL
python connectors/db_connector.py \
    --db-url "sqlite:///./data/qa.db" \
    --query "SELECT question, answer FROM qa" \
    --mapping '{"input":"question","output":"answer"}'
```

Using the connector CLI:

```bash
# Test connection
python connectors/cli.py test-connection --db-url "sqlite:///./data/qa.db"

# Stream samples
python connectors/cli.py db-stream \
    --query "SELECT question, answer FROM qa" \
    --mapping '{"input":"question","output":"answer"}'
```

### Column Mapping

The `mapping` parameter specifies which database columns correspond to training sample fields:

```python
# Direct column mapping
mapping = {"input": "question", "output": "answer"}

# With SQL aliases
query = "SELECT q AS question, a AS answer FROM faq"
mapping = {"input": "question", "output": "answer"}
```

### Output Format

Each sample includes metadata about its source:

```json
{
    "input": "How do I reset my password?",
    "output": "Go to Settings > Reset Password.",
    "metadata": {
        "source": "db",
        "raw_row": {
            "question": "How do I reset my password?",
            "answer": "Go to Settings > Reset Password."
        }
    }
}
```

### Row Mapping Utility

The `row_to_sample` function converts individual database rows to training samples:

```python
from connectors.mappers import row_to_sample

row = {"question": "What is 2+2?", "answer": "4", "category": "math"}
mapping = {"input": "question", "output": "answer"}

sample = row_to_sample(row, mapping)
```

### Error Handling

```python
from connectors.db_connector import DBConnector
from connectors.mappers import row_to_sample

# Missing column in mapping
try:
    row_to_sample(row, {"input": "nonexistent"})
except KeyError as e:
    print(f"Mapping error: {e}")

# Database connection issues
conn = DBConnector(db_url="sqlite:///nonexistent.db")
if not conn.test_connection():
    print("Connection failed")
```

### Local Development with SQLite

Create a test database for development:

```python
import sqlite3

conn = sqlite3.connect("./data/test.db")
cursor = conn.cursor()
cursor.execute("""
    CREATE TABLE qa (
        question TEXT,
        answer TEXT
    )
""")
cursor.execute("INSERT INTO qa VALUES (?, ?)",
               ("How do I reset my password?", "Go to Settings > Reset Password."))
conn.commit()
conn.close()
```

Then stream from it:

```bash
export DB_URL="sqlite:///./data/test.db"
python connectors/db_connector.py \
    --query "SELECT question, answer FROM qa" \
    --mapping '{"input":"question","output":"answer"}'
```

## Google Docs Connector

The Google Docs connector fetches and extracts text content from Google Docs. It includes a **mock mode** for offline development and testing.

### Quick Start

```python
from connectors.google_docs_connector import fetch_doc_text, list_docs_in_folder

# Fetch document text (uses mock mode by default)
text = fetch_doc_text("sample_doc1")
print(text)

# List available documents
docs = list_docs_in_folder("any_folder_id")
for doc in docs:
    print(f"{doc['id']}: {doc['title']}")
```

### Mock Mode (Default)

By default, the connector runs in mock mode (`GOOGLE_OAUTH_DISABLED=true`). In this mode, it reads from local sample files instead of making API calls.

Mock mode uses sample files from `examples/google_docs_samples/`:
- `sample_doc1.txt` - Sample documentation content
- `sample_doc2.txt` - Sample FAQ content

### Configuration

```bash
# Mock mode (default) - uses local sample files
export GOOGLE_OAUTH_DISABLED=true

# Real mode - uses Google Docs API (requires OAuth setup)
export GOOGLE_OAUTH_DISABLED=false
```

Or use the `CONNECTOR_MOCK` environment variable:

```bash
# Enable mock mode for all connectors
export CONNECTOR_MOCK=true
```

### CLI Usage

```bash
# Fetch a sample document (mock mode)
python connectors/google_docs_connector.py --doc-id sample_doc1

# List available sample documents
python connectors/google_docs_connector.py --doc-id sample_doc1 --list-samples
```

### Setting Up Google OAuth (Real Mode)

To use the connector with real Google Docs:

1. **Create a Google Cloud Project**
   - Go to [Google Cloud Console](https://console.cloud.google.com/)
   - Create a new project or select an existing one

2. **Enable the Google Docs API**
   - Navigate to "APIs & Services" > "Library"
   - Search for "Google Docs API" and enable it
   - Also enable "Google Drive API" for folder listing

3. **Create OAuth Credentials**
   - Go to "APIs & Services" > "Credentials"
   - Click "Create Credentials" > "OAuth client ID"
   - Select "Desktop application" as the application type
   - Download the credentials JSON file

4. **Install Required Dependencies**
   ```bash
   pip install google-api-python-client google-auth-httplib2 google-auth-oauthlib
   ```

5. **Set Environment Variable**
   ```bash
   export GOOGLE_OAUTH_DISABLED=false
   ```

6. **Implement OAuth Flow** (see `connectors/google_docs_connector.py` for guidance)

### Text Utilities

The connector includes utility functions for text processing:

```python
from connectors.google_utils import extract_text_from_html, normalize_text

# Strip HTML tags
html = "<p>Hello <b>world</b>!</p>"
text = extract_text_from_html(html)  # "Hello world!"

# Normalize whitespace
messy = "Too   many    spaces\n\n\n\nand lines"
clean = normalize_text(messy)  # "Too many spaces\n\nand lines"
```

### Error Handling

```python
from connectors.google_docs_connector import fetch_doc_text

try:
    text = fetch_doc_text("nonexistent_doc")
except FileNotFoundError as e:
    print(f"Document not found: {e}")
except RuntimeError as e:
    print(f"API error: {e}")
```

## File Ingestion Connector

The file ingestion connector extracts text content from various document formats.

### Quick Start

```python
from connectors.file_ingest import ingest_file

# Ingest any supported file
text = ingest_file("path/to/document.txt")
text = ingest_file("path/to/document.md")
text = ingest_file("path/to/document.docx")
text = ingest_file("path/to/document.pdf")
```

### Supported Formats

| Format | Extension | Dependency | Notes |
|--------|-----------|------------|-------|
| Plain Text | `.txt` | None (built-in) | UTF-8 encoding by default |
| Markdown | `.md` | None (built-in) | Returns raw markdown (no rendering) |
| Word Document | `.docx` | python-docx | Extracts paragraph text |
| PDF | `.pdf` | PyMuPDF or pdfminer.six | PyMuPDF preferred for speed |

### Installation

TXT and MD files work out of the box. For DOCX and PDF support, install optional dependencies:

```bash
# For DOCX support
pip install python-docx

# For PDF support (choose one)
pip install PyMuPDF        # Recommended: faster, more accurate
pip install pdfminer.six   # Alternative: pure Python
```

### Checking Available Formats

```python
from connectors.file_ingest import get_supported_formats, check_dependencies

# Check what formats are available
formats = get_supported_formats()
print(formats[".pdf"]["available"])  # True if PDF lib installed

# Check which optional dependencies are installed
deps = check_dependencies()
print(deps)
# {'python-docx': True, 'PyMuPDF': True, 'pdfminer.six': False}
```

### Custom Encoding

For TXT and MD files, you can specify a custom encoding:

```python
# Read a file with Latin-1 encoding
text = ingest_file("legacy_doc.txt", encoding="latin-1")
```

### Error Handling

```python
from connectors.file_ingest import ingest_file

try:
    text = ingest_file("document.pdf")
except FileNotFoundError as e:
    print(f"File not found: {e}")
except ValueError as e:
    print(f"Unsupported format: {e}")
except RuntimeError as e:
    print(f"Missing dependency: {e}")
```

### Sample Files

Sample files for testing are available in `examples/files/`:
- `sample.txt` - Plain text sample
- `sample.md` - Markdown sample with headings and formatting
- `sample.docx` - Word document sample (if python-docx installed)
- `sample.pdf` - PDF document sample (if PDF library installed)

## REST API Connector

The REST API connector fetches and transforms data from external REST APIs into training samples. It supports pagination, authentication, rate limiting, caching, and retry logic.

### Quick Start

```python
from connectors.api_connector import APIConnector, APIConnectorConfig

# Create connector with configuration
config = APIConnectorConfig(
    base_url="https://api.example.com",
    auth_type="bearer",
    auth_token="your-api-token",
)

connector = APIConnector(config)

# Fetch a single endpoint
data = connector.fetch("/users/1")

# Stream training samples with field mapping
mapping = {"input": "question", "output": "answer"}
for sample in connector.stream_samples("/faq", mapping):
    print(sample)
```

### Configuration

```python
from connectors.api_connector import APIConnectorConfig

config = APIConnectorConfig(
    # Base URL for all requests
    base_url="https://api.example.com",

    # Authentication (none, basic, bearer, api_key)
    auth_type="bearer",
    auth_token="your-token",

    # Or for API key authentication
    # auth_type="api_key",
    # api_key_header="X-API-Key",
    # api_key="your-key",

    # Rate limiting (requests per minute)
    rate_limit=60,

    # Retry configuration
    retry_attempts=3,
    retry_delay=1.0,  # seconds

    # Request timeout
    timeout=30,

    # Response caching
    cache_enabled=True,
    cache_ttl=300,  # seconds
)
```

### Pagination Support

The connector supports multiple pagination strategies:

```python
from connectors.api_connector import PaginationConfig, PaginationType

# Offset-based pagination
pagination = PaginationConfig(
    type=PaginationType.OFFSET,
    page_param="offset",
    limit_param="limit",
    limit=100,
)

# Page number pagination
pagination = PaginationConfig(
    type=PaginationType.PAGE,
    page_param="page",
    limit_param="per_page",
    limit=50,
)

# Cursor-based pagination
pagination = PaginationConfig(
    type=PaginationType.CURSOR,
    cursor_param="cursor",
    cursor_path="meta.next_cursor",
)

# Link header pagination (RFC 5988)
pagination = PaginationConfig(
    type=PaginationType.LINK_HEADER,
)

# Stream with pagination
for sample in connector.stream_paginated("/items", mapping, pagination):
    print(sample)
```

### Authentication Types

```python
# No authentication
config = APIConnectorConfig(base_url="https://api.example.com")

# Bearer token
config = APIConnectorConfig(
    base_url="https://api.example.com",
    auth_type="bearer",
    auth_token="your-jwt-token",
)

# Basic auth
config = APIConnectorConfig(
    base_url="https://api.example.com",
    auth_type="basic",
    auth_username="user",
    auth_password="pass",
)

# API key in header
config = APIConnectorConfig(
    base_url="https://api.example.com",
    auth_type="api_key",
    api_key_header="X-API-Key",
    api_key="your-api-key",
)
```

### Field Mapping

Map API response fields to training sample format:

```python
# Simple mapping
mapping = {"input": "question", "output": "answer"}

# Nested field access with dot notation
mapping = {"input": "data.title", "output": "data.content"}

# Transform to standard format
sample = connector.fetch_as_sample("/item/1", mapping)
# Returns: {"input": "...", "output": "...", "metadata": {...}}
```

### CLI Usage

```bash
# Fetch and display as JSON
python -m connectors.api_connector \
    --base-url "https://api.example.com" \
    --endpoint "/items" \
    --mapping '{"input": "title", "output": "body"}'

# With authentication
python -m connectors.api_connector \
    --base-url "https://api.example.com" \
    --endpoint "/items" \
    --auth-type bearer \
    --auth-token "your-token" \
    --mapping '{"input": "title", "output": "body"}'

# With pagination
python -m connectors.api_connector \
    --base-url "https://api.example.com" \
    --endpoint "/items" \
    --paginate \
    --limit 100 \
    --mapping '{"input": "title", "output": "body"}'
```

### Error Handling

```python
from connectors.api_connector import (
    APIConnector,
    APIConnectorError,
    RateLimitError,
    AuthenticationError,
)

connector = APIConnector(config)

try:
    data = connector.fetch("/endpoint")
except RateLimitError as e:
    print(f"Rate limited. Retry after {e.retry_after} seconds")
except AuthenticationError as e:
    print(f"Auth failed: {e}")
except APIConnectorError as e:
    print(f"API error: {e.status_code} - {e.message}")
```

### Caching

```python
config = APIConnectorConfig(
    base_url="https://api.example.com",
    cache_enabled=True,
    cache_ttl=300,  # Cache responses for 5 minutes
)

connector = APIConnector(config)

# First call fetches from API
data = connector.fetch("/expensive-endpoint")

# Second call returns cached response
data = connector.fetch("/expensive-endpoint")

# Force refresh
data = connector.fetch("/expensive-endpoint", force_refresh=True)

# Clear all cache
connector.clear_cache()
```

### Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `API_BASE_URL` | - | Default base URL |
| `API_AUTH_TOKEN` | - | Default bearer token |
| `API_RATE_LIMIT` | `60` | Requests per minute |
| `API_TIMEOUT` | `30` | Request timeout in seconds |
| `API_MOCK` | `false` | Enable mock mode for testing |

---

## Google Drive Connector

The Google Drive connector accesses files and folders from Google Drive. It includes a **mock mode** for offline development that reads from local sample files.

### Quick Start

```python
from connectors.google_drive_connector import GoogleDriveConnector, GoogleDriveConfig

# Create connector (uses mock mode by default)
connector = GoogleDriveConnector()

# List files in a folder
files = connector.list_files(folder_id="my_folder")
for f in files:
    print(f"{f.id}: {f.name}")

# Download file content
content = connector.get_file_content(file_id="my_file")

# Stream training samples
mapping = {"input": "input", "output": "output"}
for sample in connector.stream_samples("folder_id", mapping):
    print(sample)
```

### Configuration

```python
from connectors.google_drive_connector import GoogleDriveConfig

config = GoogleDriveConfig(
    # Authentication
    service_account_file="/path/to/service-account.json",  # For service accounts
    # Or OAuth token file
    token_file="/path/to/token.json",

    # Mock mode (default: True)
    mock_mode=False,
    samples_dir="./my_samples/",  # Custom mock samples directory

    # Query settings
    page_size=100,
    include_trashed=False,
)

connector = GoogleDriveConnector(config)
```

### Mock Mode (Default)

By default, the connector runs in mock mode (`GOOGLE_DRIVE_MOCK=true`). In this mode, it reads from local sample files in `examples/google_drive_samples/`.

```bash
# Enable mock mode (default)
export GOOGLE_DRIVE_MOCK=true

# Disable mock mode (use real Google Drive API)
export GOOGLE_DRIVE_MOCK=false
```

### CLI Usage

```bash
# List files (mock mode)
python connectors/google_drive_connector.py --list

# List files in a folder
python connectors/google_drive_connector.py --list --folder-id my_folder

# Download a file
python connectors/google_drive_connector.py --file-id sample_doc

# Stream training samples
python connectors/google_drive_connector.py --stream --folder-id my_folder \
    --mapping '{"input": "input", "output": "output"}'
```

### Setting Up Google Drive API (Real Mode)

1. **Create a Google Cloud Project** and enable the Google Drive API
2. **Create a Service Account** or OAuth credentials
3. **Download credentials** JSON file
4. **Configure the connector**:

```bash
export GOOGLE_DRIVE_MOCK=false
```

```python
config = GoogleDriveConfig(
    mock_mode=False,
    service_account_file="./credentials/service-account.json",
)
connector = GoogleDriveConnector(config)
```

### Supported File Types

The connector automatically handles Google Workspace files:

| Google Type | Export Format |
|-------------|---------------|
| Google Docs | text/plain |
| Google Sheets | text/csv |
| Google Slides | text/plain |

Native files (.txt, .json, .jsonl, .csv, .md) are downloaded as-is.

---

## Notion Connector

The Notion connector accesses pages and databases from Notion workspaces. It includes a **mock mode** for offline development.

### Quick Start

```python
from connectors.notion_connector import NotionConnector, NotionConfig

# Create connector (uses mock mode by default)
connector = NotionConnector()

# List pages in a database
pages = connector.list_pages(database_id="my_database")
for p in pages:
    print(f"{p.id}: {p.title}")

# Get page content
content = connector.get_page_content(page_id="page_id")

# Stream training samples from database
mapping = {"input": "Question", "output": "Answer"}
for sample in connector.stream_samples("database_id", mapping):
    print(sample)
```

### Configuration

```python
from connectors.notion_connector import NotionConfig

config = NotionConfig(
    # Authentication
    api_token="secret_xxx...",  # Notion integration token

    # Mock mode (default: True)
    mock_mode=False,
    samples_dir="./my_samples/",  # Custom mock samples directory

    # Query settings
    page_size=100,
    include_children=True,
    max_depth=3,
)

connector = NotionConnector(config)
```

### Mock Mode (Default)

By default, the connector runs in mock mode (`NOTION_MOCK=true`). In this mode, it reads from local sample files in `examples/notion_samples/`.

```bash
# Enable mock mode (default)
export NOTION_MOCK=true

# Disable mock mode (use real Notion API)
export NOTION_MOCK=false
export NOTION_API_TOKEN="secret_xxx..."
```

### CLI Usage

```bash
# List pages in a database (mock mode)
python connectors/notion_connector.py --list --database-id training_database

# Get page content
python connectors/notion_connector.py --page-id page-001 --content

# Stream training samples
python connectors/notion_connector.py --stream --database-id training_database \
    --mapping '{"input": "Question", "output": "Answer"}'
```

### Setting Up Notion Integration (Real Mode)

1. **Create a Notion Integration** at https://www.notion.so/my-integrations
2. **Copy the integration token** (starts with `secret_`)
3. **Share databases/pages** with your integration
4. **Configure the connector**:

```bash
export NOTION_MOCK=false
export NOTION_API_TOKEN="secret_xxx..."
```

```python
config = NotionConfig(
    mock_mode=False,
    api_token="secret_xxx...",
)
connector = NotionConnector(config)
```

### Property Types Supported

The connector extracts values from these Notion property types:

| Property Type | Extraction |
|--------------|------------|
| title | Plain text |
| rich_text | Plain text |
| number | String conversion |
| select | Selection name |
| multi_select | Comma-separated names |
| checkbox | "True"/"False" |
| url | URL string |
| email | Email string |
| date | Start date |

### Block Types Supported

When extracting page content, these block types are parsed:

- Paragraphs
- Headings (H1, H2, H3) - prefixed with #, ##, ###
- Bulleted lists - prefixed with -
- Numbered lists - prefixed with 1.
- Quotes - prefixed with >
- Code blocks - wrapped in ```

---

## Adding a New Connector

To add a new connector, follow these patterns:

### 1. Create the Connector Module

Create a new file in `connectors/`, e.g., `connectors/notion_connector.py`:

```python
"""
Notion connector for TinyForgeAI.

Provides functionality to fetch content from Notion pages and databases.
"""

import os
from typing import Iterator, Optional


def _is_mock_mode() -> bool:
    """Check if running in mock mode."""
    # Support both connector-specific and global mock env vars
    if os.getenv("NOTION_MOCK", "").lower() in ("true", "1", "yes"):
        return True
    if os.getenv("CONNECTOR_MOCK", "").lower() in ("true", "1", "yes"):
        return True
    return False


def fetch_page_content(page_id: str) -> str:
    """
    Fetch content from a Notion page.

    Args:
        page_id: The Notion page ID.

    Returns:
        The text content of the page.
    """
    if _is_mock_mode():
        return _fetch_page_mock(page_id)
    return _fetch_page_real(page_id)


def _fetch_page_mock(page_id: str) -> str:
    """Mock implementation for testing."""
    # Read from local sample files
    ...


def _fetch_page_real(page_id: str) -> str:
    """Real implementation using Notion API."""
    # Use notion-client library
    ...


def stream_samples(database_id: str, mapping: dict) -> Iterator[dict]:
    """
    Stream training samples from a Notion database.

    Args:
        database_id: The Notion database ID.
        mapping: Column mapping for input/output fields.

    Yields:
        Training sample dicts.
    """
    ...
```

### 2. Add Tests

Create `tests/test_notion_connector.py`:

```python
"""Tests for Notion connector."""

import pytest
from connectors.notion_connector import fetch_page_content


def test_fetch_page_mock_mode(monkeypatch):
    """Test fetching page in mock mode."""
    monkeypatch.setenv("NOTION_MOCK", "true")
    # Create sample file in examples/notion_samples/
    text = fetch_page_content("sample_page1")
    assert len(text) > 0


def test_fetch_page_not_found(monkeypatch):
    """Test error when page not found."""
    monkeypatch.setenv("NOTION_MOCK", "true")
    with pytest.raises(FileNotFoundError):
        fetch_page_content("nonexistent")
```

### 3. Add CLI Support

Add commands to `connectors/cli.py`:

```python
@cli.command("notion-fetch")
@click.option("--page-id", required=True, help="Notion page ID")
def notion_fetch(page_id: str):
    """Fetch content from a Notion page."""
    from connectors.notion_connector import fetch_page_content
    print(fetch_page_content(page_id))
```

### 4. Update Documentation

Add a section to this file documenting the new connector.

## Environment Variables Reference

| Variable | Default | Description |
|----------|---------|-------------|
| `DB_URL` | `sqlite:///:memory:` | Database connection URL |
| `GOOGLE_OAUTH_DISABLED` | `true` | Enable mock mode for Google Docs |
| `GOOGLE_DRIVE_MOCK` | `true` | Enable mock mode for Google Drive |
| `NOTION_MOCK` | `true` | Enable mock mode for Notion |
| `NOTION_API_TOKEN` | - | Notion integration token |
| `CONNECTOR_MOCK` | `false` | Enable mock mode for all connectors |
| `API_BASE_URL` | - | Default REST API base URL |
| `API_AUTH_TOKEN` | - | Default bearer token for API |
| `API_RATE_LIMIT` | `60` | API requests per minute |
| `API_TIMEOUT` | `30` | API request timeout in seconds |
| `API_MOCK` | `false` | Enable mock mode for API connector |

## Troubleshooting

### Database Connection Fails

```bash
# Check if database file exists
ls -la ./data/

# Test with in-memory database
python -c "from connectors.db_connector import DBConnector; print(DBConnector().test_connection())"
```

### Google Docs Mock File Not Found

```bash
# Check if sample files exist
ls -la examples/google_docs_samples/

# Verify GOOGLE_OAUTH_DISABLED is set
echo $GOOGLE_OAUTH_DISABLED
```

### PDF/DOCX Ingestion Fails

```bash
# Check which dependencies are installed
python -c "from connectors.file_ingest import check_dependencies; print(check_dependencies())"

# Install missing dependencies
pip install python-docx PyMuPDF
```

### Unicode/Encoding Errors

```python
# Specify encoding explicitly
from connectors.file_ingest import ingest_file
text = ingest_file("file.txt", encoding="utf-8-sig")  # For files with BOM
text = ingest_file("file.txt", encoding="latin-1")    # For legacy files
```

## Related Documentation

- [Training Documentation](training.md) - Using connectors with training pipeline
- [Architecture Overview](architecture.md) - System design and components
- [CI/CD](ci.md) - Continuous integration setup
