# TinyForgeAI Training Documentation
<!-- Generated by Claude Code: Step I.1 -->

**Last updated:** 2025-01-15T00:00:00Z

This document describes the training capabilities of TinyForgeAI, including dataset format, dry-run training, PEFT/LoRA integration, and model artifacts.

## Dataset Format

TinyForgeAI uses JSONL (JSON Lines) format for training data. Each line must be a valid JSON object.

### Required Fields

| Field | Type | Description |
|-------|------|-------------|
| `input` | string | The input text/prompt for the model |
| `output` | string | The expected output/response |

### Optional Fields

| Field | Type | Description |
|-------|------|-------------|
| `metadata` | object | Additional information (source, category, etc.) |

### Example JSONL File

```jsonl
{"input": "How do I reset my password?", "output": "Go to Settings > Reset Password."}
{"input": "What is your refund policy?", "output": "Refunds within 30 days with receipt."}
{"input": "Where can I find my order history?", "output": "Check your Account > Orders section."}
```

### Sample Data

A sample dataset is provided at `examples/sample_qna.jsonl`:

```bash
# View the sample data
cat examples/sample_qna.jsonl
```

```json
{"input": "How do I reset my password?", "output": "Go to Settings > Account > Reset Password and follow the instructions."}
{"input": "What is your refund policy?", "output": "We offer full refunds within 30 days of purchase with a valid receipt."}
{"input": "How can I contact support?", "output": "You can reach our support team at support@example.com or call 1-800-SUPPORT."}
```

## Loading Data Programmatically

```python
from backend.training.dataset import load_jsonl, stream_jsonl, summarize_dataset

# Load all records into memory
records = load_jsonl("examples/sample_qna.jsonl")
print(f"Loaded {len(records)} records")

# Stream records for large files (memory efficient)
for record in stream_jsonl("examples/sample_qna.jsonl"):
    print(record["input"][:50])

# Get dataset statistics
summary = summarize_dataset(records)
print(f"Records: {summary['n_records']}")
print(f"Avg input length: {summary['avg_input_len']:.2f} tokens")
print(f"Avg output length: {summary['avg_output_len']:.2f} tokens")
```

## Dry-Run Training

The dry-run trainer validates your dataset and creates a stub model artifact without performing actual training. This is useful for:

- Validating dataset format and content
- Testing the training pipeline end-to-end
- Generating placeholder artifacts for export testing

### Running Dry-Run Training

```bash
# Basic dry-run
python backend/training/train.py --data examples/sample_qna.jsonl --out /tmp/tiny_model --dry-run
```

### CLI Flags

| Flag | Description |
|------|-------------|
| `--data` | Path to JSONL training data file (required) |
| `--out` | Output directory for model artifacts (required) |
| `--dry-run` | Validate data and create stub artifact only |
| `--use-lora` | Apply LoRA adapter to the model |

### Using the foremforge CLI

```bash
# Dry-run training with CLI
foremforge train --data examples/sample_qna.jsonl --out /tmp/tiny_model --dry-run

# With LoRA adapter
foremforge train --data examples/sample_qna.jsonl --out /tmp/tiny_model --dry-run --use-lora
```

### Output

The trainer produces:

```
/tmp/tiny_model/
└── model_stub.json    # Model metadata and configuration
```

Example `model_stub.json`:

```json
{
  "model_type": "tinyforge_stub",
  "n_records": 3,
  "created_time": "2025-01-15T12:00:00+00:00",
  "notes": "dry-run artifact"
}
```

## PEFT / LoRA Hook

### What is PEFT/LoRA?

PEFT (Parameter-Efficient Fine-Tuning) is a family of techniques for fine-tuning large language models using fewer trainable parameters. LoRA (Low-Rank Adaptation) is a popular PEFT method that injects trainable low-rank matrices into transformer layers, enabling efficient fine-tuning while preserving most of the original model weights.

### TinyForgeAI LoRA Stub

The `apply_lora(model, config)` function simulates LoRA adapter application during dry-run training. This allows you to test your training pipeline with LoRA configuration before integrating a real PEFT implementation.

### Usage

**CLI with LoRA:**

```bash
python backend/training/train.py --data examples/sample_qna.jsonl --out /tmp/tiny_model --use-lora --dry-run
```

**Programmatic usage:**

```python
from backend.training.peft_adapter import apply_lora

model = {"model_type": "tinyforge_stub", "n_records": 3}
patched = apply_lora(model)

# patched now contains:
# - lora_applied: True
# - lora_config: {"r": 8, "alpha": 16, "target_modules": ["q", "v"]}
# - lora_timestamp: ISO8601 UTC timestamp
```

### Default LoRA Configuration

| Parameter | Default | Description |
|-----------|---------|-------------|
| `r` | 8 | LoRA rank (low-rank dimension) |
| `alpha` | 16 | Scaling factor |
| `target_modules` | `["q", "v"]` | Transformer modules to apply LoRA to |

### Output with LoRA

When `--use-lora` is enabled, the `model_stub.json` includes LoRA metadata:

```json
{
  "model_type": "tinyforge_stub",
  "n_records": 3,
  "created_time": "2025-01-15T12:00:00+00:00",
  "notes": "dry-run artifact",
  "lora_applied": true,
  "lora_config": {
    "r": 8,
    "alpha": 16,
    "target_modules": ["q", "v"]
  },
  "lora_timestamp": "2025-01-15T12:00:00+00:00"
}
```

## How to Replace Stub with Real Training

To integrate real model training with Hugging Face Transformers and PEFT:

### Checklist

1. **Pick a Base Model**
   - Choose a suitable model (e.g., `t5-small`, `gpt2`, `llama-2-7b`)
   - Consider model size vs. available compute resources

2. **Prepare Tokenization and Dataset Mapping**
   ```python
   from transformers import AutoTokenizer
   from datasets import Dataset

   tokenizer = AutoTokenizer.from_pretrained("t5-small")

   def tokenize_function(examples):
       inputs = tokenizer(examples["input"], truncation=True, max_length=512)
       targets = tokenizer(examples["output"], truncation=True, max_length=128)
       inputs["labels"] = targets["input_ids"]
       return inputs
   ```

3. **Use PEFT (LoRA) or Full Fine-Tune**
   ```python
   from peft import LoraConfig, get_peft_model

   lora_config = LoraConfig(
       r=8,
       lora_alpha=16,
       target_modules=["q", "v"],
       lora_dropout=0.1,
   )
   model = get_peft_model(base_model, lora_config)
   ```

4. **Convert Adapters/Weights into Deployable Artifact**
   ```python
   # Save LoRA adapters
   model.save_pretrained("./lora_adapters")

   # For deployment, merge weights
   merged_model = model.merge_and_unload()
   merged_model.save_pretrained("./merged_model")
   ```

5. **Add Test/Validation Holdout and Monitoring**
   - Split data: 80% train, 10% validation, 10% test
   - Track metrics: loss, perplexity, task-specific metrics
   - Use early stopping based on validation performance

## Artifacts & Exports

### Model Stub (model_stub.json)

Created by dry-run trainer:

```json
{
  "model_type": "tinyforge_stub",
  "n_records": 3,
  "created_time": "2025-01-15T12:00:00+00:00",
  "notes": "dry-run artifact"
}
```

### ONNX Placeholder (model.onnx)

Created by `--export-onnx` flag:

```
# TinyForgeAI ONNX placeholder
# Exported: 2025-01-15T12:00:00+00:00
# Source: /tmp/tiny_model/model_stub.json
# This is a stub file for testing purposes.
```

With metadata in `model.onnx.meta.json`:

```json
{
  "exported_from": "/tmp/tiny_model/model_stub.json",
  "export_time": "2025-01-15T12:00:00+00:00",
  "version": "0.1-stub",
  "source_model": {
    "model_type": "tinyforge_stub",
    "n_records": 3
  }
}
```

### Quantized Placeholder (quantized.onnx)

Created by quantization hook:

```json
{
  "exported_from": "/tmp/tiny_model/model_stub.json",
  "export_time": "2025-01-15T12:00:00+00:00",
  "version": "0.1-stub",
  "quantized": true,
  "mode": "int8",
  "quantize_time": "2025-01-15T12:00:01+00:00"
}
```

### Model Metadata (model_metadata.json)

Created by the exporter builder in the microservice output:

```json
{
  "model_path": "/tmp/tiny_model/model_stub.json",
  "created_time": "2025-01-15T12:00:00+00:00",
  "source": "tinyforge-exporter",
  "model_stub": true
}
```

### Export Report (export_report.json)

Created when `--export-onnx` is used:

```json
{
  "onnx_path": "/tmp/tiny_service/onnx/model.onnx",
  "quantized_path": "/tmp/tiny_service/onnx/quant/quantized.onnx",
  "export_time": "2025-01-15T12:00:00+00:00",
  "model_path": "/tmp/tiny_model/model_stub.json"
}
```

## Complete Example Workflow

```bash
# 1. Train (dry-run)
python backend/training/train.py \
  --data examples/sample_qna.jsonl \
  --out /tmp/tiny_model \
  --dry-run \
  --use-lora

# 2. Export with ONNX
python backend/exporter/builder.py \
  --model-path /tmp/tiny_model/model_stub.json \
  --output-dir /tmp/tiny_service \
  --overwrite \
  --export-onnx

# 3. Inspect artifacts
ls -la /tmp/tiny_model/
cat /tmp/tiny_model/model_stub.json

ls -la /tmp/tiny_service/
cat /tmp/tiny_service/model_metadata.json

ls -la /tmp/tiny_service/onnx/
cat /tmp/tiny_service/export_report.json
```

## Related Documentation

- [Architecture Overview](architecture.md) - System design and components
- [Data Connectors](connectors.md) - Loading data from various sources
- [CI/CD](ci.md) - Continuous integration setup
