{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"announcement_article/","title":"TinyForgeAI \u2014 Tiny Models, Big Impact","text":"<p>Train private, fast, and deployable language models from your own data</p>"},{"location":"announcement_article/#the-problem-with-large-language-models","title":"The Problem with Large Language Models","text":"<p>Large language models are powerful, but they're not always the right tool for the job.</p> <p>The reality for most businesses: - You don't need 70 billion parameters to answer questions about your company's refund policy - Cloud API costs add up quickly for high-volume use cases - Sensitive data can't always be sent to external APIs - Latency requirements often exceed what cloud services can deliver - Compliance regulations may mandate on-premise solutions</p> <p>What most companies actually need: - A small, focused model trained on their specific data - Fast inference times (milliseconds, not seconds) - Complete data privacy and control - Predictable costs with no per-request fees - Easy deployment to existing infrastructure</p>"},{"location":"announcement_article/#introducing-tinyforgeai","title":"Introducing TinyForgeAI","text":"<p>TinyForgeAI is an open-source framework that lets you train tiny language models from your own data and deploy them as REST microservices \u2014 all from the command line.</p> <p>No ML expertise required. No cloud dependencies. No massive GPU clusters.</p>"},{"location":"announcement_article/#how-it-works","title":"How It Works","text":"<pre><code>Your Data \u2192 Train \u2192 Export \u2192 Deploy \u2192 Serve\n</code></pre> <ol> <li>Ingest your data from databases, files, or Google Docs</li> <li>Train a tiny model on your specific use case</li> <li>Export as a microservice with one command</li> <li>Deploy anywhere \u2014 Docker, Kubernetes, or bare metal</li> </ol>"},{"location":"announcement_article/#three-commands-to-production","title":"Three Commands to Production","text":"<pre><code># Train a model from your data\nforemforge train --data your_data.jsonl --out ./model --dry-run\n\n# Export as a microservice\nforemforge export --model ./model/model_stub.json --out ./service\n\n# Deploy and serve\nforemforge serve --dir ./service --port 8000\n</code></pre> <p>That's it. You now have a REST API answering questions based on your data.</p>"},{"location":"announcement_article/#architecture-overview","title":"Architecture Overview","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        TinyForgeAI                          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                             \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n\u2502   \u2502  CLI    \u2502\u2500\u2500\u2500\u2500\u25b6\u2502  Connectors  \u2502\u2500\u2500\u2500\u2500\u25b6\u2502   Trainer    \u2502    \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502              \u2502     \u2502              \u2502    \u2502\n\u2502                   \u2502 \u2022 Files      \u2502     \u2502 \u2022 Dry-run    \u2502    \u2502\n\u2502                   \u2502 \u2022 Database   \u2502     \u2502 \u2022 LoRA stub  \u2502    \u2502\n\u2502                   \u2502 \u2022 Google Docs\u2502     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n\u2502                   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2502            \u2502\n\u2502                                               \u25bc            \u2502\n\u2502                                      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502\n\u2502                                      \u2502   Exporter   \u2502      \u2502\n\u2502                                      \u2502              \u2502      \u2502\n\u2502                                      \u2502 \u2022 FastAPI    \u2502      \u2502\n\u2502                                      \u2502 \u2022 Docker     \u2502      \u2502\n\u2502                                      \u2502 \u2022 ONNX       \u2502      \u2502\n\u2502                                      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502\n\u2502                                              \u2502             \u2502\n\u2502                                              \u25bc             \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502\n\u2502   \u2502 Your Apps   \u2502\u25c0\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502  Inference   \u2502      \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                   \u2502   Server     \u2502      \u2502\n\u2502                                      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502\n\u2502                                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"announcement_article/#key-features","title":"Key Features","text":""},{"location":"announcement_article/#multiple-data-connectors","title":"Multiple Data Connectors","text":"<p>TinyForgeAI ingests data from the sources you already use:</p> Source Status Notes Local Files \u2705 PDF, DOCX, TXT, JSON, JSONL SQLite/PostgreSQL \u2705 Streaming reader, batch mode Google Docs \u2705 Mock mode for testing REST APIs \ud83d\udd1c Coming soon"},{"location":"announcement_article/#production-ready-export","title":"Production-Ready Export","text":"<p>Every exported service includes: - FastAPI application with <code>/predict</code>, <code>/health</code>, <code>/readyz</code> endpoints - Dockerfile optimized for production - docker-compose.yml for easy deployment - Kubernetes manifests and Helm charts</p>"},{"location":"announcement_article/#dry-run-mode","title":"Dry-Run Mode","text":"<p>Test your entire pipeline without training real models:</p> <pre><code>foremforge train --data data.jsonl --out ./model --dry-run\n</code></pre> <p>Dry-run mode: - Validates your data format - Creates stub model artifacts - Tests the full export pipeline - Verifies your deployment configuration</p> <p>Perfect for CI/CD pipelines and rapid iteration.</p>"},{"location":"announcement_article/#real-world-use-cases","title":"Real-World Use Cases","text":""},{"location":"announcement_article/#internal-support-bot","title":"Internal Support Bot","text":"<p>Train on your knowledge base, deploy behind your firewall:</p> <pre><code>foremforge train --data support_articles.jsonl --out ./support_model\nforemforge export --model ./support_model/model_stub.json --out ./support_service\ndocker-compose up -d\n</code></pre>"},{"location":"announcement_article/#document-qa-system","title":"Document Q&amp;A System","text":"<p>Turn your PDFs and Word documents into a queryable API:</p> <pre><code>foremforge ingest --input ./documents/ --output training_data.jsonl\nforemforge train --data training_data.jsonl --out ./doc_model\n</code></pre>"},{"location":"announcement_article/#workflow-automation","title":"Workflow Automation","text":"<p>Embed inference directly in your automation pipelines:</p> <pre><code>import requests\n\nresponse = requests.post(\n    \"http://localhost:8000/predict\",\n    json={\"input\": \"What's the status of order #12345?\"}\n)\nprint(response.json()[\"output\"])\n</code></pre>"},{"location":"announcement_article/#why-not-just-use-gpt-4claude","title":"Why Not Just Use GPT-4/Claude?","text":"Factor Cloud LLMs TinyForgeAI Data Privacy \u274c Data leaves your network \u2705 100% on-premise Cost \ud83d\udcb0 Per-request pricing \ud83d\udcb0 Fixed infrastructure Latency \ud83d\udc0c 100-2000ms \u26a1 &lt;50ms Customization \u274c Generic responses \u2705 Your data, your model Offline \u274c Requires internet \u2705 Works anywhere Compliance \u26a0\ufe0f May violate policies \u2705 Full control"},{"location":"announcement_article/#getting-started","title":"Getting Started","text":""},{"location":"announcement_article/#installation","title":"Installation","text":"<pre><code>pip install tinyforgeai\n</code></pre> <p>Or from source:</p> <pre><code>git clone https://github.com/anthropics/TinyForgeAI.git\ncd TinyForgeAI\npip install -e \".[dev]\"\n</code></pre>"},{"location":"announcement_article/#run-the-demo","title":"Run the Demo","text":"<pre><code>python examples/e2e_demo.py\n</code></pre> <p>This will: 1. Train a stub model on sample data 2. Export it as a FastAPI service 3. Test the <code>/predict</code> endpoint</p>"},{"location":"announcement_article/#next-steps","title":"Next Steps","text":"<ol> <li>Read the documentation</li> <li>Try the connectors</li> <li>Deploy to Kubernetes</li> </ol>"},{"location":"announcement_article/#whats-next","title":"What's Next","text":"<p>TinyForgeAI v0.1.0 is the foundation. Here's what's coming:</p> <ul> <li>Real Training Engine: HuggingFace Transformers + LoRA fine-tuning</li> <li>Model Zoo Expansion: Pre-trained models for common tasks</li> <li>Web Dashboard: Visual training and deployment management</li> <li>RAG Integration: Retrieval-augmented generation for better accuracy</li> <li>Multi-tenant Inference: Serve multiple models from one service</li> </ul>"},{"location":"announcement_article/#contributing","title":"Contributing","text":"<p>TinyForgeAI is open source under Apache 2.0. We welcome contributions!</p> <ul> <li>GitHub Repository</li> <li>Contributing Guide</li> <li>Issue Tracker</li> </ul> <p>TinyForgeAI \u2014 Because sometimes, smaller is smarter.</p> <p>Train tiny models. Deploy anywhere. Own your AI.</p>"},{"location":"announcement_article/#about","title":"About","text":"<p>TinyForgeAI is developed by FOREM as an open-source project. We believe AI should be accessible, private, and deployable anywhere.</p> <p>Links: - GitHub: https://github.com/anthropics/TinyForgeAI - Documentation: https://github.com/anthropics/TinyForgeAI/tree/main/docs - License: Apache 2.0</p>"},{"location":"api_reference/","title":"API Reference \u2014 TinyForgeAI Inference Microservice","text":"<p>This document describes the REST API endpoints provided by TinyForgeAI inference services.</p>"},{"location":"api_reference/#base-url","title":"Base URL","text":"<pre><code>http://localhost:8000\n</code></pre>"},{"location":"api_reference/#authentication","title":"Authentication","text":"<p>Currently, TinyForgeAI services do not require authentication. For production deployments, consider adding authentication via a reverse proxy (nginx, Traefik) or API gateway.</p>"},{"location":"api_reference/#endpoints","title":"Endpoints","text":""},{"location":"api_reference/#post-predict","title":"POST /predict","text":"<p>Perform inference on input text.</p>"},{"location":"api_reference/#request","title":"Request","text":"<pre><code>POST /predict HTTP/1.1\nHost: localhost:8000\nContent-Type: application/json\n\n{\n  \"input\": \"string\"\n}\n</code></pre>"},{"location":"api_reference/#request-body","title":"Request Body","text":"Field Type Required Description <code>input</code> string Yes The input text for inference <code>metadata</code> object No Optional metadata to include with request"},{"location":"api_reference/#response","title":"Response","text":"<pre><code>{\n  \"output\": \"string\",\n  \"confidence\": 0.75\n}\n</code></pre>"},{"location":"api_reference/#response-fields","title":"Response Fields","text":"Field Type Description <code>output</code> string The model's output/prediction <code>confidence</code> float Confidence score (0.0 - 1.0)"},{"location":"api_reference/#example","title":"Example","text":"<p>Request:</p> <pre><code>curl -X POST http://localhost:8000/predict \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"input\": \"hello world\"}'\n</code></pre> <p>Response:</p> <pre><code>{\n  \"output\": \"dlrow olleh\",\n  \"confidence\": 0.75\n}\n</code></pre>"},{"location":"api_reference/#error-responses","title":"Error Responses","text":"Status Description 400 Bad Request - Invalid JSON or missing required fields 422 Unprocessable Entity - Validation error 500 Internal Server Error"},{"location":"api_reference/#get-health","title":"GET /health","text":"<p>Health check endpoint for liveness probes.</p>"},{"location":"api_reference/#request_1","title":"Request","text":"<pre><code>GET /health HTTP/1.1\nHost: localhost:8000\n</code></pre>"},{"location":"api_reference/#response_1","title":"Response","text":"<pre><code>{\n  \"status\": \"healthy\",\n  \"uptime_seconds\": 3600.5\n}\n</code></pre>"},{"location":"api_reference/#response-fields_1","title":"Response Fields","text":"Field Type Description <code>status</code> string Health status (\"healthy\") <code>uptime_seconds</code> float Server uptime in seconds"},{"location":"api_reference/#example_1","title":"Example","text":"<pre><code>curl http://localhost:8000/health\n</code></pre>"},{"location":"api_reference/#get-readyz","title":"GET /readyz","text":"<p>Readiness probe endpoint. Returns 200 when service is ready to accept requests.</p>"},{"location":"api_reference/#request_2","title":"Request","text":"<pre><code>GET /readyz HTTP/1.1\nHost: localhost:8000\n</code></pre>"},{"location":"api_reference/#response_2","title":"Response","text":"<pre><code>{\n  \"ready\": true\n}\n</code></pre>"},{"location":"api_reference/#response-fields_2","title":"Response Fields","text":"Field Type Description <code>ready</code> boolean Whether service is ready"},{"location":"api_reference/#example_2","title":"Example","text":"<pre><code>curl http://localhost:8000/readyz\n</code></pre>"},{"location":"api_reference/#get-metrics","title":"GET /metrics","text":"<p>Basic metrics endpoint.</p>"},{"location":"api_reference/#request_3","title":"Request","text":"<pre><code>GET /metrics HTTP/1.1\nHost: localhost:8000\n</code></pre>"},{"location":"api_reference/#response_3","title":"Response","text":"<pre><code>{\n  \"request_count\": 1247,\n  \"uptime_seconds\": 3600.5\n}\n</code></pre>"},{"location":"api_reference/#response-fields_3","title":"Response Fields","text":"Field Type Description <code>request_count</code> integer Total number of prediction requests <code>uptime_seconds</code> float Server uptime in seconds"},{"location":"api_reference/#example_3","title":"Example","text":"<pre><code>curl http://localhost:8000/metrics\n</code></pre>"},{"location":"api_reference/#get-openapijson","title":"GET /openapi.json","text":"<p>OpenAPI 3.0 specification for the API.</p>"},{"location":"api_reference/#request_4","title":"Request","text":"<pre><code>GET /openapi.json HTTP/1.1\nHost: localhost:8000\n</code></pre>"},{"location":"api_reference/#response_4","title":"Response","text":"<p>Returns the full OpenAPI specification in JSON format.</p>"},{"location":"api_reference/#get-docs","title":"GET /docs","text":"<p>Interactive API documentation (Swagger UI).</p>"},{"location":"api_reference/#request_5","title":"Request","text":"<p>Open in browser: <pre><code>http://localhost:8000/docs\n</code></pre></p>"},{"location":"api_reference/#get-redoc","title":"GET /redoc","text":"<p>Alternative API documentation (ReDoc).</p>"},{"location":"api_reference/#request_6","title":"Request","text":"<p>Open in browser: <pre><code>http://localhost:8000/redoc\n</code></pre></p>"},{"location":"api_reference/#openapi-specification","title":"OpenAPI Specification","text":"<pre><code>openapi: \"3.0.0\"\ninfo:\n  title: \"TinyForgeAI Inference API\"\n  version: \"0.1.0\"\n  description: \"REST API for TinyForgeAI inference services\"\n  license:\n    name: \"Apache 2.0\"\n    url: \"https://www.apache.org/licenses/LICENSE-2.0\"\n\nservers:\n  - url: \"http://localhost:8000\"\n    description: \"Local development server\"\n\npaths:\n  /predict:\n    post:\n      summary: \"Perform inference\"\n      description: \"Send input text and receive model prediction\"\n      operationId: \"predict\"\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema:\n              type: object\n              required:\n                - input\n              properties:\n                input:\n                  type: string\n                  description: \"Input text for inference\"\n                  example: \"hello world\"\n                metadata:\n                  type: object\n                  description: \"Optional metadata\"\n      responses:\n        \"200\":\n          description: \"Successful prediction\"\n          content:\n            application/json:\n              schema:\n                type: object\n                properties:\n                  output:\n                    type: string\n                    description: \"Model output\"\n                  confidence:\n                    type: number\n                    format: float\n                    description: \"Confidence score (0-1)\"\n        \"400\":\n          description: \"Bad request\"\n        \"500\":\n          description: \"Server error\"\n\n  /health:\n    get:\n      summary: \"Health check\"\n      description: \"Returns service health status\"\n      operationId: \"health\"\n      responses:\n        \"200\":\n          description: \"Service is healthy\"\n          content:\n            application/json:\n              schema:\n                type: object\n                properties:\n                  status:\n                    type: string\n                    example: \"healthy\"\n                  uptime_seconds:\n                    type: number\n                    format: float\n\n  /readyz:\n    get:\n      summary: \"Readiness check\"\n      description: \"Returns whether service is ready\"\n      operationId: \"readyz\"\n      responses:\n        \"200\":\n          description: \"Service is ready\"\n          content:\n            application/json:\n              schema:\n                type: object\n                properties:\n                  ready:\n                    type: boolean\n\n  /metrics:\n    get:\n      summary: \"Get metrics\"\n      description: \"Returns basic service metrics\"\n      operationId: \"metrics\"\n      responses:\n        \"200\":\n          description: \"Metrics response\"\n          content:\n            application/json:\n              schema:\n                type: object\n                properties:\n                  request_count:\n                    type: integer\n                  uptime_seconds:\n                    type: number\n                    format: float\n</code></pre>"},{"location":"api_reference/#sdk-examples","title":"SDK Examples","text":""},{"location":"api_reference/#python","title":"Python","text":"<pre><code>import requests\n\n# Basic prediction\nresponse = requests.post(\n    \"http://localhost:8000/predict\",\n    json={\"input\": \"hello world\"}\n)\nresult = response.json()\nprint(f\"Output: {result['output']}\")\nprint(f\"Confidence: {result['confidence']:.2%}\")\n\n# With error handling\ntry:\n    response = requests.post(\n        \"http://localhost:8000/predict\",\n        json={\"input\": \"test\"},\n        timeout=5\n    )\n    response.raise_for_status()\n    print(response.json())\nexcept requests.exceptions.RequestException as e:\n    print(f\"Error: {e}\")\n</code></pre>"},{"location":"api_reference/#javascripttypescript","title":"JavaScript/TypeScript","text":"<pre><code>// Using fetch\nconst response = await fetch('http://localhost:8000/predict', {\n  method: 'POST',\n  headers: { 'Content-Type': 'application/json' },\n  body: JSON.stringify({ input: 'hello world' })\n});\n\nconst result = await response.json();\nconsole.log(`Output: ${result.output}`);\nconsole.log(`Confidence: ${(result.confidence * 100).toFixed(1)}%`);\n</code></pre>"},{"location":"api_reference/#curl","title":"cURL","text":"<pre><code># Prediction\ncurl -X POST http://localhost:8000/predict \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"input\": \"hello world\"}'\n\n# Health check\ncurl http://localhost:8000/health\n\n# Metrics\ncurl http://localhost:8000/metrics\n</code></pre>"},{"location":"api_reference/#error-handling","title":"Error Handling","text":""},{"location":"api_reference/#error-response-format","title":"Error Response Format","text":"<pre><code>{\n  \"detail\": \"Error description\"\n}\n</code></pre>"},{"location":"api_reference/#common-errors","title":"Common Errors","text":"Status Code Meaning Resolution 400 Bad Request Check request body format 404 Not Found Verify endpoint URL 422 Validation Error Check required fields 500 Server Error Check server logs 503 Service Unavailable Wait and retry"},{"location":"api_reference/#rate-limiting","title":"Rate Limiting","text":"<p>Default TinyForgeAI services do not implement rate limiting. For production deployments, consider:</p> <ol> <li>Reverse Proxy: Configure rate limiting in nginx/Traefik</li> <li>API Gateway: Use AWS API Gateway, Kong, or similar</li> <li>Application Level: Add FastAPI middleware</li> </ol>"},{"location":"api_reference/#see-also","title":"See Also","text":"<ul> <li>Deployment Guide</li> <li>Architecture</li> <li>Playground</li> </ul>"},{"location":"architecture/","title":"TinyForgeAI Architecture","text":"<p>Last updated: 2025-01-15T00:00:00Z</p>"},{"location":"architecture/#overview","title":"Overview","text":"<p>TinyForgeAI is a lightweight platform for fine-tuning language models and deploying them as inference microservices. The system is designed with simplicity and developer experience in mind, enabling rapid prototyping of ML-powered applications without heavy infrastructure dependencies.</p> <p>The platform follows a modular architecture where each component can operate independently or as part of an integrated pipeline. Data flows from various sources through connectors, gets processed by the training pipeline, and results in deployable model artifacts that can be served via containerized inference servers.</p> <p>TinyForgeAI emphasizes offline-first development with mock modes for all external integrations, making it possible to develop and test the entire pipeline without network access or cloud credentials. The system produces lightweight artifacts suitable for CPU inference, with optional ONNX export and quantization for optimized deployment.</p> <p>The CLI tool (<code>foremforge</code>) provides a unified interface for all platform operations, from project initialization to model serving, while the underlying Python APIs remain accessible for programmatic use and custom integrations.</p>"},{"location":"architecture/#architecture-diagram","title":"Architecture Diagram","text":"<pre><code>flowchart TB\n    subgraph Input[\"Data Sources\"]\n        CLI[CLI / foremforge]\n        API[Backend API]\n        Files[Local Files]\n    end\n\n    subgraph Connectors[\"Data Connector Layer\"]\n        DB[Database Connector]\n        GDocs[Google Docs Connector]\n        FileIngest[File Ingestion]\n    end\n\n    subgraph Training[\"Training Pipeline\"]\n        Dataset[Dataset Loader]\n        Trainer[Training Orchestrator]\n        PEFT[PEFT/LoRA Adapter]\n    end\n\n    subgraph Storage[\"Model Registry\"]\n        Artifacts[Model Artifacts]\n        Metadata[Metadata JSON]\n        ONNX[ONNX Models]\n    end\n\n    subgraph Export[\"Exporter/Packaging\"]\n        Builder[Microservice Builder]\n        Quantize[Quantization Hook]\n        Docker[Docker Packaging]\n    end\n\n    subgraph Serving[\"Inference Layer\"]\n        Server[Inference Server]\n        Health[Health Endpoints]\n        Predict[Predict API]\n    end\n\n    subgraph Optional[\"Optional Components\"]\n        RAG[RAG / Vector DB]\n        Monitor[Monitoring]\n    end\n\n    CLI --&gt; API\n    API --&gt; Connectors\n    Files --&gt; FileIngest\n\n    Connectors --&gt; Dataset\n    Dataset --&gt; Trainer\n    Trainer --&gt; PEFT\n    PEFT --&gt; Artifacts\n\n    Artifacts --&gt; Metadata\n    Artifacts --&gt; ONNX\n\n    Artifacts --&gt; Builder\n    ONNX --&gt; Quantize\n    Builder --&gt; Docker\n\n    Docker --&gt; Server\n    Server --&gt; Health\n    Server --&gt; Predict\n\n    Trainer -.-&gt; RAG\n    Server -.-&gt; Monitor\n</code></pre>"},{"location":"architecture/#train-export-serve-pipeline","title":"Train \u2192 Export \u2192 Serve Pipeline","text":"<p>The complete workflow from raw data to serving predictions follows these steps:</p>"},{"location":"architecture/#step-1-prepare-training-data","title":"Step 1: Prepare Training Data","text":"<p>Create a JSONL file with input/output pairs:</p> <pre><code># View sample data format\ncat examples/sample_qna.jsonl\n</code></pre>"},{"location":"architecture/#step-2-train-the-model-dry-run","title":"Step 2: Train the Model (Dry-Run)","text":"<p>Run the training pipeline to validate data and create model artifacts:</p> <pre><code># Basic dry-run training\npython backend/training/train.py --data examples/sample_qna.jsonl --out /tmp/tiny_model --dry-run\n\n# With LoRA adapter\npython backend/training/train.py --data examples/sample_qna.jsonl --out /tmp/tiny_model --dry-run --use-lora\n</code></pre> <p>This creates <code>/tmp/tiny_model/model_stub.json</code> with model metadata.</p>"},{"location":"architecture/#step-3-export-to-microservice","title":"Step 3: Export to Microservice","text":"<p>Package the model into a deployable inference service:</p> <pre><code># Basic export\npython backend/exporter/builder.py --model-path /tmp/tiny_model/model_stub.json --output-dir /tmp/tiny_service --overwrite\n\n# With ONNX export and quantization\npython backend/exporter/builder.py --model-path /tmp/tiny_model/model_stub.json --output-dir /tmp/tiny_service --overwrite --export-onnx\n</code></pre> <p>This creates a complete microservice in <code>/tmp/tiny_service/</code>.</p>"},{"location":"architecture/#step-4-serve-the-model","title":"Step 4: Serve the Model","text":"<p>Start the inference server:</p> <pre><code># Using uvicorn directly\ncd /tmp/tiny_service\nuvicorn app:app --host 0.0.0.0 --port 8001\n\n# Or using the CLI\nforemforge serve --dir /tmp/tiny_service --port 8001\n</code></pre>"},{"location":"architecture/#step-5-make-predictions","title":"Step 5: Make Predictions","text":"<p>Query the running server:</p> <pre><code># Health check\ncurl -sS http://127.0.0.1:8001/health\n\n# Make a prediction\ncurl -sS -X POST http://127.0.0.1:8001/predict \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"text\":\"hello\"}'\n</code></pre>"},{"location":"architecture/#using-the-cli-recommended","title":"Using the CLI (Recommended)","text":"<p>The <code>foremforge</code> CLI simplifies the entire workflow:</p> <pre><code># Initialize project structure\nforemforge init --yes\n\n# Train\nforemforge train --data examples/sample_qna.jsonl --out /tmp/tiny_model --dry-run\n\n# Export with ONNX\nforemforge export --model /tmp/tiny_model/model_stub.json --out /tmp/tiny_service --overwrite --export-onnx\n\n# Serve\nforemforge serve --dir /tmp/tiny_service --port 8001\n</code></pre>"},{"location":"architecture/#run-the-demo","title":"Run the Demo","text":"<p>Try the complete workflow with our end-to-end demo scripts:</p> <pre><code># Using bash (Linux/macOS/Git Bash on Windows)\nbash examples/e2e_demo.sh\n\n# Using Python (cross-platform)\npython examples/e2e_demo.py\n</code></pre> <p>The demo performs all steps automatically: 1. Creates a temporary workspace 2. Copies sample training data 3. Runs dry-run training to create <code>model_stub.json</code> 4. Exports to an inference microservice 5. Tests the <code>/predict</code> endpoint using FastAPI TestClient</p> <p>Expected output:</p> <pre><code>[Step 4] Running smoke test against generated service...\n  -&gt; Inference response:\n{\n  \"output\": \"olleh\",\n  \"confidence\": 0.75\n}\n\n==================================================\nDemo Complete!\n==================================================\n\nSummary:\n  Model path:     /tmp/xxx/tiny_model/model_stub.json\n  Service path:   /tmp/xxx/service\n  Response:       {\"output\": \"olleh\", \"confidence\": 0.75}\n</code></pre> <p>Use <code>--cleanup</code> to automatically delete the workspace when done.</p>"},{"location":"architecture/#operational-concerns","title":"Operational Concerns","text":""},{"location":"architecture/#security","title":"Security","text":"<ul> <li>API Keys: Never commit credentials to the repository. Use environment variables or <code>.env</code> files (excluded from git).</li> <li>Private Data: Training data may contain sensitive information. Ensure proper access controls on data directories and model artifacts.</li> <li>Network Exposure: The inference server binds to <code>0.0.0.0</code> by default. In production, use a reverse proxy (nginx) with TLS.</li> <li>Docker Security: The Dockerfile runs as a non-root user. Ensure mounted volumes have appropriate permissions.</li> </ul>"},{"location":"architecture/#resource-considerations","title":"Resource Considerations","text":"<ul> <li>CPU Inference: The stub implementation is lightweight. Real model inference may require significant CPU/memory.</li> <li>Quantization: Use INT8 quantization (<code>--export-onnx</code>) to reduce model size and improve CPU inference speed.</li> <li>Memory: Monitor memory usage during training. Large datasets should be streamed rather than loaded entirely.</li> <li>Disk Space: Model artifacts and ONNX files can be large. Implement cleanup policies for old artifacts.</li> </ul>"},{"location":"architecture/#monitoring-and-logging","title":"Monitoring and Logging","text":"<p>Key monitoring points:</p> Component Endpoint/Log What to Monitor Inference Server <code>/health</code> Uptime, response time Inference Server <code>/predict</code> Latency, error rate, throughput Training stdout logs Progress, validation metrics Docker Container logs Startup errors, OOM events <p>Logging configuration:</p> <pre><code># Set log level via environment variable\nLOG_LEVEL=DEBUG python backend/training/train.py ...\n</code></pre>"},{"location":"architecture/#where-to-find-the-code","title":"Where to Find the Code","text":"Component Path Description Backend API <code>backend/api/</code> FastAPI application with health routes Training Pipeline <code>backend/training/</code> Dataset loader, trainer, PEFT adapter Model Exporter <code>backend/exporter/</code> Builder, ONNX export, quantization Data Connectors <code>connectors/</code> DB, Google Docs, file ingestion Inference Server <code>inference_server/</code> FastAPI inference service template CLI Tool <code>cli/</code> foremforge Click-based CLI Docker Config <code>docker/</code> Dockerfile and docker-compose Tests <code>tests/</code> pytest test suite Documentation <code>docs/</code> This documentation Examples <code>examples/</code> Sample data files"},{"location":"architecture/#related-documentation","title":"Related Documentation","text":"<ul> <li>Training Pipeline - Detailed training documentation</li> <li>Data Connectors - Connector configuration and usage</li> <li>CI/CD - Continuous integration setup</li> <li>Docker Guide - Container deployment</li> </ul>"},{"location":"ci/","title":"TinyForgeAI CI/CD Documentation","text":"<p>This document describes the Continuous Integration (CI) setup for TinyForgeAI.</p>"},{"location":"ci/#overview","title":"Overview","text":"<p>The CI pipeline runs automatically on: - Push to <code>main</code> or <code>master</code> branches - Pull requests to any branch</p> <p>The pipeline consists of three jobs: 1. test - Runs the pytest test suite 2. lint - Runs flake8 linter for code quality 3. docker-build - Builds the Docker image (validation only, no push)</p>"},{"location":"ci/#github-actions-workflow","title":"GitHub Actions Workflow","text":"<p>The CI workflow is defined in <code>.github/workflows/ci.yml</code>.</p>"},{"location":"ci/#test-job","title":"Test Job","text":"<p>Runs the full pytest test suite: - Uses Python 3.10 - Installs dependencies from <code>requirements.txt</code> - Runs <code>pytest -q</code></p>"},{"location":"ci/#lint-job","title":"Lint Job","text":"<p>Runs flake8 linting: - Checks for syntax errors and undefined names (fails on these) - Reports style issues as warnings (does not fail) - Ignores long lines (E501) and line break issues (W503)</p>"},{"location":"ci/#docker-build-job","title":"Docker Build Job","text":"<p>Builds the Docker image to validate the Dockerfile: - Uses Docker Buildx for efficient builds - Does NOT push images to any registry - Uses GitHub Actions cache for faster builds - Continues on error if Dockerfile doesn't exist</p>"},{"location":"ci/#running-ci-locally","title":"Running CI Locally","text":"<p>You can run the same CI checks locally before pushing.</p>"},{"location":"ci/#using-make-recommended","title":"Using Make (Recommended)","text":"<pre><code># Run tests only\nmake test\n\n# Run tests with verbose output\nmake test-verbose\n\n# Run linter only\nmake lint\n\n# Run full CI pipeline (test + lint + docker)\nmake ci-local\n</code></pre>"},{"location":"ci/#using-pytest-directly","title":"Using pytest directly","text":"<pre><code># Quick test run\npytest -q\n\n# Verbose test run\npytest -v\n\n# Run specific test file\npytest tests/test_cli.py -v\n\n# Run with coverage (if installed)\npytest --cov=backend --cov=cli --cov=connectors\n</code></pre>"},{"location":"ci/#using-flake8-directly","title":"Using flake8 directly","text":"<pre><code># Check for syntax errors (strict)\nflake8 . --select=E9,F63,F7,F82 --show-source\n\n# Full lint check\nflake8 . --max-line-length=100 --ignore=E501,W503\n</code></pre>"},{"location":"ci/#building-docker-locally","title":"Building Docker locally","text":"<pre><code># Build the inference server image\ndocker build -f docker/Dockerfile -t foremsoft/tinyforgeai:local .\n\n# Or using make\nmake docker-build\n</code></pre>"},{"location":"ci/#environment-variables","title":"Environment Variables","text":"<p>The following environment variables affect CI behavior:</p> Variable Description Default <code>PYTHON_VERSION</code> Python version for CI <code>3.10</code> <code>GOOGLE_OAUTH_DISABLED</code> Disable Google OAuth (mock mode) <code>true</code>"},{"location":"ci/#adding-repository-secrets","title":"Adding Repository Secrets","text":"<p>For future features that require secrets (e.g., Docker Hub push, deployment):</p> <ol> <li>Go to your GitHub repository</li> <li>Navigate to Settings &gt; Secrets and variables &gt; Actions</li> <li>Click New repository secret</li> <li>Add the secret name and value</li> </ol> <p>Common secrets you might need: - <code>DOCKERHUB_USERNAME</code> - Docker Hub username - <code>DOCKERHUB_TOKEN</code> - Docker Hub access token - <code>PYPI_API_TOKEN</code> - PyPI API token for package publishing</p> <p>Note: Never commit secrets to the repository. Always use GitHub Secrets.</p>"},{"location":"ci/#troubleshooting","title":"Troubleshooting","text":""},{"location":"ci/#tests-failing-locally-but-passing-in-ci","title":"Tests failing locally but passing in CI","text":"<ul> <li>Ensure you have the same Python version (3.10)</li> <li>Install all dependencies: <code>pip install -r requirements.txt</code></li> <li>Check for platform-specific issues (Windows vs Linux)</li> </ul>"},{"location":"ci/#lint-errors","title":"Lint errors","text":"<ul> <li>Run <code>make lint</code> to see all issues</li> <li>Most style issues are warnings and won't fail the build</li> <li>Syntax errors (E9, F63, F7, F82) will fail the build</li> </ul>"},{"location":"ci/#docker-build-failing","title":"Docker build failing","text":"<ul> <li>Ensure <code>docker/Dockerfile</code> exists</li> <li>Check that all required files are present</li> <li>Review Docker build logs for specific errors</li> </ul>"},{"location":"ci/#ci-status-badge","title":"CI Status Badge","text":"<p>Add this badge to your README to show CI status:</p> <pre><code>![CI](https://github.com/foremsoft/TinyForgeAI/actions/workflows/ci.yml/badge.svg)\n</code></pre>"},{"location":"connectors/","title":"TinyForgeAI Connectors Documentation","text":"<p>Last updated: 2025-01-15T00:00:00Z</p> <p>This document describes the data source connectors available in TinyForgeAI for loading training data from various sources.</p>"},{"location":"connectors/#overview","title":"Overview","text":"<p>The Data Connector Layer provides a unified interface for ingesting training data from multiple sources. Each connector follows a consistent pattern:</p> <ul> <li>Programmatic API: Python classes and functions for integration into custom pipelines</li> <li>CLI Support: Command-line tools for quick data extraction and testing</li> <li>Mock Mode: Offline development support via environment variables</li> </ul>"},{"location":"connectors/#supported-data-sources","title":"Supported Data Sources","text":"Source Connector Status Mock Mode SQL Databases <code>DBConnector</code> Implemented N/A (use SQLite) Google Docs <code>google_docs_connector</code> Implemented <code>GOOGLE_OAUTH_DISABLED=true</code> Local Files <code>file_ingest</code> Implemented N/A Google Drive Planned - - Notion Planned - - REST APIs Planned - -"},{"location":"connectors/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     Data Sources                            \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502 SQL DB  \u2502  \u2502 Google Docs \u2502  \u2502 Files     \u2502  \u2502 API/etc  \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        \u2502              \u2502               \u2502             \u2502\n        \u25bc              \u25bc               \u25bc             \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                   Connector Layer                           \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502DBConnector  \u2502 \u2502google_docs_connect\u2502 \u2502file_ingest      \u2502 \u2502\n\u2502  \u2502             \u2502 \u2502                   \u2502 \u2502                 \u2502 \u2502\n\u2502  \u2502stream_sample\u2502 \u2502fetch_doc_text()   \u2502 \u2502ingest_file()    \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n          \u2502                  \u2502                    \u2502\n          \u25bc                  \u25bc                    \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                  Training Samples (JSONL)                   \u2502\n\u2502       {\"input\": \"...\", \"output\": \"...\", \"metadata\": {...}}  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"connectors/#database-connector","title":"Database Connector","text":"<p>The database connector streams training samples directly from SQL databases. It uses Python's built-in <code>sqlite3</code> module for SQLite databases.</p>"},{"location":"connectors/#quick-start","title":"Quick Start","text":"<pre><code>from connectors.db_connector import DBConnector\n\n# Create connector (uses DB_URL from environment by default)\nconn = DBConnector(db_url=\"sqlite:///./data/qa.db\")\n\n# Test connection\nif conn.test_connection():\n    print(\"Connected successfully!\")\n\n# Stream training samples\nquery = \"SELECT question, answer FROM qa_pairs\"\nmapping = {\"input\": \"question\", \"output\": \"answer\"}\n\nfor sample in conn.stream_samples(query, mapping):\n    print(sample)\n</code></pre>"},{"location":"connectors/#configuration","title":"Configuration","text":"<p>Database connection settings are configured via environment variables or the <code>DBSettings</code> class:</p> <pre><code># SQLite file database\nexport DB_URL=\"sqlite:///./data/training.db\"\n\n# In-memory SQLite (default)\nexport DB_URL=\"sqlite:///:memory:\"\n</code></pre> <p>Or in your <code>.env</code> file:</p> <pre><code>DB_URL=sqlite:///./data/training.db\n</code></pre>"},{"location":"connectors/#cli-usage","title":"CLI Usage","text":"<pre><code># Stream samples as JSONL\npython connectors/db_connector.py \\\n    --query \"SELECT question AS q, answer AS a FROM qa\" \\\n    --mapping '{\"input\":\"q\",\"output\":\"a\"}'\n\n# Limit output\npython connectors/db_connector.py \\\n    --query \"SELECT question, answer FROM qa\" \\\n    --mapping '{\"input\":\"question\",\"output\":\"answer\"}' \\\n    --limit 10\n\n# Specify database URL\npython connectors/db_connector.py \\\n    --db-url \"sqlite:///./data/qa.db\" \\\n    --query \"SELECT question, answer FROM qa\" \\\n    --mapping '{\"input\":\"question\",\"output\":\"answer\"}'\n</code></pre> <p>Using the connector CLI:</p> <pre><code># Test connection\npython connectors/cli.py test-connection --db-url \"sqlite:///./data/qa.db\"\n\n# Stream samples\npython connectors/cli.py db-stream \\\n    --query \"SELECT question, answer FROM qa\" \\\n    --mapping '{\"input\":\"question\",\"output\":\"answer\"}'\n</code></pre>"},{"location":"connectors/#column-mapping","title":"Column Mapping","text":"<p>The <code>mapping</code> parameter specifies which database columns correspond to training sample fields:</p> <pre><code># Direct column mapping\nmapping = {\"input\": \"question\", \"output\": \"answer\"}\n\n# With SQL aliases\nquery = \"SELECT q AS question, a AS answer FROM faq\"\nmapping = {\"input\": \"question\", \"output\": \"answer\"}\n</code></pre>"},{"location":"connectors/#output-format","title":"Output Format","text":"<p>Each sample includes metadata about its source:</p> <pre><code>{\n    \"input\": \"How do I reset my password?\",\n    \"output\": \"Go to Settings &gt; Reset Password.\",\n    \"metadata\": {\n        \"source\": \"db\",\n        \"raw_row\": {\n            \"question\": \"How do I reset my password?\",\n            \"answer\": \"Go to Settings &gt; Reset Password.\"\n        }\n    }\n}\n</code></pre>"},{"location":"connectors/#row-mapping-utility","title":"Row Mapping Utility","text":"<p>The <code>row_to_sample</code> function converts individual database rows to training samples:</p> <pre><code>from connectors.mappers import row_to_sample\n\nrow = {\"question\": \"What is 2+2?\", \"answer\": \"4\", \"category\": \"math\"}\nmapping = {\"input\": \"question\", \"output\": \"answer\"}\n\nsample = row_to_sample(row, mapping)\n</code></pre>"},{"location":"connectors/#error-handling","title":"Error Handling","text":"<pre><code>from connectors.db_connector import DBConnector\nfrom connectors.mappers import row_to_sample\n\n# Missing column in mapping\ntry:\n    row_to_sample(row, {\"input\": \"nonexistent\"})\nexcept KeyError as e:\n    print(f\"Mapping error: {e}\")\n\n# Database connection issues\nconn = DBConnector(db_url=\"sqlite:///nonexistent.db\")\nif not conn.test_connection():\n    print(\"Connection failed\")\n</code></pre>"},{"location":"connectors/#local-development-with-sqlite","title":"Local Development with SQLite","text":"<p>Create a test database for development:</p> <pre><code>import sqlite3\n\nconn = sqlite3.connect(\"./data/test.db\")\ncursor = conn.cursor()\ncursor.execute(\"\"\"\n    CREATE TABLE qa (\n        question TEXT,\n        answer TEXT\n    )\n\"\"\")\ncursor.execute(\"INSERT INTO qa VALUES (?, ?)\",\n               (\"How do I reset my password?\", \"Go to Settings &gt; Reset Password.\"))\nconn.commit()\nconn.close()\n</code></pre> <p>Then stream from it:</p> <pre><code>export DB_URL=\"sqlite:///./data/test.db\"\npython connectors/db_connector.py \\\n    --query \"SELECT question, answer FROM qa\" \\\n    --mapping '{\"input\":\"question\",\"output\":\"answer\"}'\n</code></pre>"},{"location":"connectors/#google-docs-connector","title":"Google Docs Connector","text":"<p>The Google Docs connector fetches and extracts text content from Google Docs. It includes a mock mode for offline development and testing.</p>"},{"location":"connectors/#quick-start_1","title":"Quick Start","text":"<pre><code>from connectors.google_docs_connector import fetch_doc_text, list_docs_in_folder\n\n# Fetch document text (uses mock mode by default)\ntext = fetch_doc_text(\"sample_doc1\")\nprint(text)\n\n# List available documents\ndocs = list_docs_in_folder(\"any_folder_id\")\nfor doc in docs:\n    print(f\"{doc['id']}: {doc['title']}\")\n</code></pre>"},{"location":"connectors/#mock-mode-default","title":"Mock Mode (Default)","text":"<p>By default, the connector runs in mock mode (<code>GOOGLE_OAUTH_DISABLED=true</code>). In this mode, it reads from local sample files instead of making API calls.</p> <p>Mock mode uses sample files from <code>examples/google_docs_samples/</code>: - <code>sample_doc1.txt</code> - Sample documentation content - <code>sample_doc2.txt</code> - Sample FAQ content</p>"},{"location":"connectors/#configuration_1","title":"Configuration","text":"<pre><code># Mock mode (default) - uses local sample files\nexport GOOGLE_OAUTH_DISABLED=true\n\n# Real mode - uses Google Docs API (requires OAuth setup)\nexport GOOGLE_OAUTH_DISABLED=false\n</code></pre> <p>Or use the <code>CONNECTOR_MOCK</code> environment variable:</p> <pre><code># Enable mock mode for all connectors\nexport CONNECTOR_MOCK=true\n</code></pre>"},{"location":"connectors/#cli-usage_1","title":"CLI Usage","text":"<pre><code># Fetch a sample document (mock mode)\npython connectors/google_docs_connector.py --doc-id sample_doc1\n\n# List available sample documents\npython connectors/google_docs_connector.py --doc-id sample_doc1 --list-samples\n</code></pre>"},{"location":"connectors/#setting-up-google-oauth-real-mode","title":"Setting Up Google OAuth (Real Mode)","text":"<p>To use the connector with real Google Docs:</p> <ol> <li>Create a Google Cloud Project</li> <li>Go to Google Cloud Console</li> <li> <p>Create a new project or select an existing one</p> </li> <li> <p>Enable the Google Docs API</p> </li> <li>Navigate to \"APIs &amp; Services\" &gt; \"Library\"</li> <li>Search for \"Google Docs API\" and enable it</li> <li> <p>Also enable \"Google Drive API\" for folder listing</p> </li> <li> <p>Create OAuth Credentials</p> </li> <li>Go to \"APIs &amp; Services\" &gt; \"Credentials\"</li> <li>Click \"Create Credentials\" &gt; \"OAuth client ID\"</li> <li>Select \"Desktop application\" as the application type</li> <li> <p>Download the credentials JSON file</p> </li> <li> <p>Install Required Dependencies <pre><code>pip install google-api-python-client google-auth-httplib2 google-auth-oauthlib\n</code></pre></p> </li> <li> <p>Set Environment Variable <pre><code>export GOOGLE_OAUTH_DISABLED=false\n</code></pre></p> </li> <li> <p>Implement OAuth Flow (see <code>connectors/google_docs_connector.py</code> for guidance)</p> </li> </ol>"},{"location":"connectors/#text-utilities","title":"Text Utilities","text":"<p>The connector includes utility functions for text processing:</p> <pre><code>from connectors.google_utils import extract_text_from_html, normalize_text\n\n# Strip HTML tags\nhtml = \"&lt;p&gt;Hello &lt;b&gt;world&lt;/b&gt;!&lt;/p&gt;\"\ntext = extract_text_from_html(html)  # \"Hello world!\"\n\n# Normalize whitespace\nmessy = \"Too   many    spaces\\n\\n\\n\\nand lines\"\nclean = normalize_text(messy)  # \"Too many spaces\\n\\nand lines\"\n</code></pre>"},{"location":"connectors/#error-handling_1","title":"Error Handling","text":"<pre><code>from connectors.google_docs_connector import fetch_doc_text\n\ntry:\n    text = fetch_doc_text(\"nonexistent_doc\")\nexcept FileNotFoundError as e:\n    print(f\"Document not found: {e}\")\nexcept RuntimeError as e:\n    print(f\"API error: {e}\")\n</code></pre>"},{"location":"connectors/#file-ingestion-connector","title":"File Ingestion Connector","text":"<p>The file ingestion connector extracts text content from various document formats.</p>"},{"location":"connectors/#quick-start_2","title":"Quick Start","text":"<pre><code>from connectors.file_ingest import ingest_file\n\n# Ingest any supported file\ntext = ingest_file(\"path/to/document.txt\")\ntext = ingest_file(\"path/to/document.md\")\ntext = ingest_file(\"path/to/document.docx\")\ntext = ingest_file(\"path/to/document.pdf\")\n</code></pre>"},{"location":"connectors/#supported-formats","title":"Supported Formats","text":"Format Extension Dependency Notes Plain Text <code>.txt</code> None (built-in) UTF-8 encoding by default Markdown <code>.md</code> None (built-in) Returns raw markdown (no rendering) Word Document <code>.docx</code> python-docx Extracts paragraph text PDF <code>.pdf</code> PyMuPDF or pdfminer.six PyMuPDF preferred for speed"},{"location":"connectors/#installation","title":"Installation","text":"<p>TXT and MD files work out of the box. For DOCX and PDF support, install optional dependencies:</p> <pre><code># For DOCX support\npip install python-docx\n\n# For PDF support (choose one)\npip install PyMuPDF        # Recommended: faster, more accurate\npip install pdfminer.six   # Alternative: pure Python\n</code></pre>"},{"location":"connectors/#checking-available-formats","title":"Checking Available Formats","text":"<pre><code>from connectors.file_ingest import get_supported_formats, check_dependencies\n\n# Check what formats are available\nformats = get_supported_formats()\nprint(formats[\".pdf\"][\"available\"])  # True if PDF lib installed\n\n# Check which optional dependencies are installed\ndeps = check_dependencies()\nprint(deps)\n# {'python-docx': True, 'PyMuPDF': True, 'pdfminer.six': False}\n</code></pre>"},{"location":"connectors/#custom-encoding","title":"Custom Encoding","text":"<p>For TXT and MD files, you can specify a custom encoding:</p> <pre><code># Read a file with Latin-1 encoding\ntext = ingest_file(\"legacy_doc.txt\", encoding=\"latin-1\")\n</code></pre>"},{"location":"connectors/#error-handling_2","title":"Error Handling","text":"<pre><code>from connectors.file_ingest import ingest_file\n\ntry:\n    text = ingest_file(\"document.pdf\")\nexcept FileNotFoundError as e:\n    print(f\"File not found: {e}\")\nexcept ValueError as e:\n    print(f\"Unsupported format: {e}\")\nexcept RuntimeError as e:\n    print(f\"Missing dependency: {e}\")\n</code></pre>"},{"location":"connectors/#sample-files","title":"Sample Files","text":"<p>Sample files for testing are available in <code>examples/files/</code>: - <code>sample.txt</code> - Plain text sample - <code>sample.md</code> - Markdown sample with headings and formatting - <code>sample.docx</code> - Word document sample (if python-docx installed) - <code>sample.pdf</code> - PDF document sample (if PDF library installed)</p>"},{"location":"connectors/#adding-a-new-connector","title":"Adding a New Connector","text":"<p>To add a new connector, follow these patterns:</p>"},{"location":"connectors/#1-create-the-connector-module","title":"1. Create the Connector Module","text":"<p>Create a new file in <code>connectors/</code>, e.g., <code>connectors/notion_connector.py</code>:</p> <pre><code>\"\"\"\nNotion connector for TinyForgeAI.\n\nProvides functionality to fetch content from Notion pages and databases.\n\"\"\"\n\nimport os\nfrom typing import Iterator, Optional\n\n\ndef _is_mock_mode() -&gt; bool:\n    \"\"\"Check if running in mock mode.\"\"\"\n    # Support both connector-specific and global mock env vars\n    if os.getenv(\"NOTION_MOCK\", \"\").lower() in (\"true\", \"1\", \"yes\"):\n        return True\n    if os.getenv(\"CONNECTOR_MOCK\", \"\").lower() in (\"true\", \"1\", \"yes\"):\n        return True\n    return False\n\n\ndef fetch_page_content(page_id: str) -&gt; str:\n    \"\"\"\n    Fetch content from a Notion page.\n\n    Args:\n        page_id: The Notion page ID.\n\n    Returns:\n        The text content of the page.\n    \"\"\"\n    if _is_mock_mode():\n        return _fetch_page_mock(page_id)\n    return _fetch_page_real(page_id)\n\n\ndef _fetch_page_mock(page_id: str) -&gt; str:\n    \"\"\"Mock implementation for testing.\"\"\"\n    # Read from local sample files\n    ...\n\n\ndef _fetch_page_real(page_id: str) -&gt; str:\n    \"\"\"Real implementation using Notion API.\"\"\"\n    # Use notion-client library\n    ...\n\n\ndef stream_samples(database_id: str, mapping: dict) -&gt; Iterator[dict]:\n    \"\"\"\n    Stream training samples from a Notion database.\n\n    Args:\n        database_id: The Notion database ID.\n        mapping: Column mapping for input/output fields.\n\n    Yields:\n        Training sample dicts.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"connectors/#2-add-tests","title":"2. Add Tests","text":"<p>Create <code>tests/test_notion_connector.py</code>:</p> <pre><code>\"\"\"Tests for Notion connector.\"\"\"\n\nimport pytest\nfrom connectors.notion_connector import fetch_page_content\n\n\ndef test_fetch_page_mock_mode(monkeypatch):\n    \"\"\"Test fetching page in mock mode.\"\"\"\n    monkeypatch.setenv(\"NOTION_MOCK\", \"true\")\n    # Create sample file in examples/notion_samples/\n    text = fetch_page_content(\"sample_page1\")\n    assert len(text) &gt; 0\n\n\ndef test_fetch_page_not_found(monkeypatch):\n    \"\"\"Test error when page not found.\"\"\"\n    monkeypatch.setenv(\"NOTION_MOCK\", \"true\")\n    with pytest.raises(FileNotFoundError):\n        fetch_page_content(\"nonexistent\")\n</code></pre>"},{"location":"connectors/#3-add-cli-support","title":"3. Add CLI Support","text":"<p>Add commands to <code>connectors/cli.py</code>:</p> <pre><code>@cli.command(\"notion-fetch\")\n@click.option(\"--page-id\", required=True, help=\"Notion page ID\")\ndef notion_fetch(page_id: str):\n    \"\"\"Fetch content from a Notion page.\"\"\"\n    from connectors.notion_connector import fetch_page_content\n    print(fetch_page_content(page_id))\n</code></pre>"},{"location":"connectors/#4-update-documentation","title":"4. Update Documentation","text":"<p>Add a section to this file documenting the new connector.</p>"},{"location":"connectors/#environment-variables-reference","title":"Environment Variables Reference","text":"Variable Default Description <code>DB_URL</code> <code>sqlite:///:memory:</code> Database connection URL <code>GOOGLE_OAUTH_DISABLED</code> <code>true</code> Enable mock mode for Google Docs <code>CONNECTOR_MOCK</code> <code>false</code> Enable mock mode for all connectors"},{"location":"connectors/#troubleshooting","title":"Troubleshooting","text":""},{"location":"connectors/#database-connection-fails","title":"Database Connection Fails","text":"<pre><code># Check if database file exists\nls -la ./data/\n\n# Test with in-memory database\npython -c \"from connectors.db_connector import DBConnector; print(DBConnector().test_connection())\"\n</code></pre>"},{"location":"connectors/#google-docs-mock-file-not-found","title":"Google Docs Mock File Not Found","text":"<pre><code># Check if sample files exist\nls -la examples/google_docs_samples/\n\n# Verify GOOGLE_OAUTH_DISABLED is set\necho $GOOGLE_OAUTH_DISABLED\n</code></pre>"},{"location":"connectors/#pdfdocx-ingestion-fails","title":"PDF/DOCX Ingestion Fails","text":"<pre><code># Check which dependencies are installed\npython -c \"from connectors.file_ingest import check_dependencies; print(check_dependencies())\"\n\n# Install missing dependencies\npip install python-docx PyMuPDF\n</code></pre>"},{"location":"connectors/#unicodeencoding-errors","title":"Unicode/Encoding Errors","text":"<pre><code># Specify encoding explicitly\nfrom connectors.file_ingest import ingest_file\ntext = ingest_file(\"file.txt\", encoding=\"utf-8-sig\")  # For files with BOM\ntext = ingest_file(\"file.txt\", encoding=\"latin-1\")    # For legacy files\n</code></pre>"},{"location":"connectors/#related-documentation","title":"Related Documentation","text":"<ul> <li>Training Documentation - Using connectors with training pipeline</li> <li>Architecture Overview - System design and components</li> <li>CI/CD - Continuous integration setup</li> </ul>"},{"location":"developer_onboarding/","title":"TinyForgeAI \u2014 Developer Onboarding Guide","text":"<p>Last updated: December 2024</p> <p>Welcome to TinyForgeAI! This guide will help you get up to speed with the codebase and start contributing.</p>"},{"location":"developer_onboarding/#1-repository-structure","title":"1. Repository Structure","text":"<pre><code>TinyForgeAI/\n\u251c\u2500\u2500 backend/\n\u2502   \u251c\u2500\u2500 dataset/           # JSONL schema and validation\n\u2502   \u2502   \u2514\u2500\u2500 schema.py      # Pydantic models for training data\n\u2502   \u251c\u2500\u2500 trainer/           # Training pipeline\n\u2502   \u2502   \u2514\u2500\u2500 stub_trainer.py # Dry-run trainer implementation\n\u2502   \u251c\u2500\u2500 exporter/          # Model export and packaging\n\u2502   \u2502   \u251c\u2500\u2500 builder.py     # Microservice generator\n\u2502   \u2502   \u2514\u2500\u2500 onnx_export.py # ONNX export utility\n\u2502   \u2514\u2500\u2500 api/               # FastAPI application (future)\n\u251c\u2500\u2500 connectors/            # Data source connectors\n\u2502   \u251c\u2500\u2500 db_connector.py    # SQLite/PostgreSQL/MySQL\n\u2502   \u251c\u2500\u2500 google_docs_connector.py # Google Docs (mock mode)\n\u2502   \u251c\u2500\u2500 google_utils.py    # OAuth utilities\n\u2502   \u251c\u2500\u2500 file_ingest.py     # File ingestion (PDF/DOCX/TXT)\n\u2502   \u2514\u2500\u2500 file_helpers.py    # PDF/DOCX extraction helpers\n\u251c\u2500\u2500 inference_server/      # Inference service template\n\u2502   \u2514\u2500\u2500 server.py          # FastAPI inference server\n\u251c\u2500\u2500 cli/                   # CLI tool\n\u2502   \u2514\u2500\u2500 foremforge.py      # Click-based CLI commands\n\u251c\u2500\u2500 docker/                # Docker configurations\n\u2502   \u251c\u2500\u2500 Dockerfile.inference\n\u2502   \u251c\u2500\u2500 docker-compose.yml\n\u2502   \u2514\u2500\u2500 README.md\n\u251c\u2500\u2500 examples/              # Sample data and demos\n\u2502   \u251c\u2500\u2500 data/              # Sample JSONL datasets\n\u2502   \u251c\u2500\u2500 e2e_demo.sh        # Bash demo script\n\u2502   \u2514\u2500\u2500 e2e_demo.py        # Python demo script\n\u251c\u2500\u2500 docs/                  # Documentation\n\u251c\u2500\u2500 tests/                 # Test suite (215+ tests)\n\u251c\u2500\u2500 MODEL_ZOO/             # Pre-built model examples\n\u2514\u2500\u2500 releases/              # Release scripts and notes\n</code></pre>"},{"location":"developer_onboarding/#2-getting-started","title":"2. Getting Started","text":""},{"location":"developer_onboarding/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10+</li> <li>pip or poetry</li> <li>Git</li> </ul>"},{"location":"developer_onboarding/#setup","title":"Setup","text":"<pre><code># Clone the repository\ngit clone https://github.com/anthropics/TinyForgeAI.git\ncd TinyForgeAI\n\n# Create virtual environment (recommended)\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n\n# Install with dev dependencies\npip install -e \".[dev]\"\n\n# Verify installation\nforemforge --help\npytest -q\n</code></pre>"},{"location":"developer_onboarding/#3-running-the-full-flow-locally","title":"3. Running the Full Flow Locally","text":""},{"location":"developer_onboarding/#train-export-test","title":"Train \u2192 Export \u2192 Test","text":"<pre><code># Step 1: Train (dry-run mode)\nforemforge train \\\n    --data examples/data/demo_dataset.jsonl \\\n    --out tmp/model \\\n    --dry-run\n\n# Step 2: Export as microservice\nforemforge export \\\n    --model tmp/model/model_stub.json \\\n    --out tmp/service \\\n    --overwrite \\\n    --export-onnx\n\n# Step 3: Test the service\npython examples/e2e_demo.py\n</code></pre>"},{"location":"developer_onboarding/#using-the-python-api","title":"Using the Python API","text":"<pre><code>from backend.trainer.stub_trainer import StubTrainer\nfrom backend.exporter.builder import build\n\n# Train\ntrainer = StubTrainer(data_path=\"examples/data/demo_dataset.jsonl\")\ntrainer.train(output_dir=\"tmp/model\", dry_run=True)\n\n# Export\nbuild(\n    model_path=\"tmp/model/model_stub.json\",\n    output_dir=\"tmp/service\",\n    overwrite=True,\n)\n\n# Test\nfrom fastapi.testclient import TestClient\nimport importlib.util\nimport pathlib\n\nspec = importlib.util.spec_from_file_location(\n    \"service_app\",\n    pathlib.Path(\"tmp/service/app.py\")\n)\nmod = importlib.util.module_from_spec(spec)\nspec.loader.exec_module(mod)\n\nclient = TestClient(mod.app)\nresponse = client.post(\"/predict\", json={\"input\": \"hello\"})\nprint(response.json())  # {\"output\": \"olleh\", \"confidence\": 0.75}\n</code></pre>"},{"location":"developer_onboarding/#4-adding-a-new-connector","title":"4. Adding a New Connector","text":"<p>Connectors ingest data from various sources and convert them to the training format.</p>"},{"location":"developer_onboarding/#required-interface","title":"Required Interface","text":"<pre><code># connectors/my_connector.py\n\nfrom typing import Iterator, Dict, Any, Optional\nfrom dataclasses import dataclass\n\n@dataclass\nclass MyConnectorConfig:\n    \"\"\"Configuration for MyConnector.\"\"\"\n    source: str\n    mock_mode: bool = False\n    # Add other config options\n\nclass MyConnector:\n    \"\"\"Connector for [data source description].\"\"\"\n\n    def __init__(self, config: MyConnectorConfig):\n        self.config = config\n        self._validate_config()\n\n    def _validate_config(self) -&gt; None:\n        \"\"\"Validate configuration.\"\"\"\n        if not self.config.source:\n            raise ValueError(\"source is required\")\n\n    def stream_samples(self) -&gt; Iterator[Dict[str, Any]]:\n        \"\"\"\n        Stream samples from the data source.\n\n        Yields:\n            Dict with keys: input, output, metadata (optional)\n        \"\"\"\n        if self.config.mock_mode:\n            yield from self._mock_samples()\n        else:\n            yield from self._real_samples()\n\n    def _mock_samples(self) -&gt; Iterator[Dict[str, Any]]:\n        \"\"\"Generate mock samples for testing.\"\"\"\n        yield {\n            \"input\": \"mock input\",\n            \"output\": \"mock output\",\n            \"metadata\": {\"source\": \"mock\"}\n        }\n\n    def _real_samples(self) -&gt; Iterator[Dict[str, Any]]:\n        \"\"\"Generate real samples from the data source.\"\"\"\n        # Implement actual data fetching\n        pass\n\n    @staticmethod\n    def row_to_sample(row: Any) -&gt; Dict[str, Any]:\n        \"\"\"Convert a raw row to training sample format.\"\"\"\n        return {\n            \"input\": str(row.get(\"question\", \"\")),\n            \"output\": str(row.get(\"answer\", \"\")),\n            \"metadata\": {\"raw\": row}\n        }\n</code></pre>"},{"location":"developer_onboarding/#testing-your-connector","title":"Testing Your Connector","text":"<pre><code># tests/test_my_connector.py\n\nimport pytest\nfrom connectors.my_connector import MyConnector, MyConnectorConfig\n\nclass TestMyConnector:\n    def test_mock_mode(self):\n        config = MyConnectorConfig(source=\"test\", mock_mode=True)\n        connector = MyConnector(config)\n\n        samples = list(connector.stream_samples())\n        assert len(samples) &gt; 0\n        assert \"input\" in samples[0]\n        assert \"output\" in samples[0]\n\n    def test_invalid_config(self):\n        with pytest.raises(ValueError):\n            MyConnectorConfig(source=\"\")\n</code></pre>"},{"location":"developer_onboarding/#5-adding-a-new-exporter-format","title":"5. Adding a New Exporter Format","text":"<p>Exporters convert trained models to deployment formats.</p>"},{"location":"developer_onboarding/#required-interface_1","title":"Required Interface","text":"<pre><code># backend/exporter/my_format_export.py\n\nfrom pathlib import Path\nfrom typing import Dict, Any, Optional\n\ndef export_to_my_format(\n    model_path: Path,\n    output_path: Path,\n    config: Optional[Dict[str, Any]] = None\n) -&gt; Path:\n    \"\"\"\n    Export model to MyFormat.\n\n    Args:\n        model_path: Path to model artifact (model_stub.json)\n        output_path: Where to save the exported model\n        config: Optional export configuration\n\n    Returns:\n        Path to the exported model file\n    \"\"\"\n    # Load model metadata\n    import json\n    with open(model_path) as f:\n        metadata = json.load(f)\n\n    # Perform export\n    output_file = output_path / \"model.myformat\"\n\n    # Write exported model\n    with open(output_file, \"w\") as f:\n        # Export logic here\n        pass\n\n    return output_file\n\ndef quantize_my_format(\n    model_path: Path,\n    output_path: Path,\n    bits: int = 8\n) -&gt; Path:\n    \"\"\"\n    Quantize a MyFormat model.\n\n    Args:\n        model_path: Path to MyFormat model\n        output_path: Where to save quantized model\n        bits: Quantization bit width (4, 8, 16)\n\n    Returns:\n        Path to quantized model\n    \"\"\"\n    pass\n</code></pre>"},{"location":"developer_onboarding/#integration-with-builder","title":"Integration with Builder","text":"<p>Add to <code>backend/exporter/builder.py</code>:</p> <pre><code>def build(..., export_my_format: bool = False):\n    # ... existing code ...\n\n    if export_my_format:\n        from backend.exporter.my_format_export import export_to_my_format\n        export_to_my_format(model_path, output_dir)\n</code></pre>"},{"location":"developer_onboarding/#testing","title":"Testing","text":"<pre><code># tests/test_export_my_format.py\n\nimport pytest\nfrom pathlib import Path\nfrom backend.exporter.my_format_export import export_to_my_format\n\nclass TestMyFormatExport:\n    def test_export(self, tmp_path):\n        # Create mock model\n        model_path = tmp_path / \"model_stub.json\"\n        model_path.write_text('{\"model_type\": \"test\"}')\n\n        # Export\n        output = export_to_my_format(model_path, tmp_path)\n\n        assert output.exists()\n</code></pre>"},{"location":"developer_onboarding/#6-training-real-mode","title":"6. Training (Real Mode)","text":"<p>The current implementation uses a stub trainer. To implement real training:</p>"},{"location":"developer_onboarding/#step-1-add-huggingface-transformers","title":"Step 1: Add HuggingFace Transformers","text":"<pre><code># backend/trainer/hf_trainer.py\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\nfrom peft import LoraConfig, get_peft_model\nfrom datasets import load_dataset\n\nclass HFTrainer:\n    def __init__(self, model_name: str = \"gpt2\"):\n        self.model_name = model_name\n        self.model = None\n        self.tokenizer = None\n\n    def load_model(self):\n        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name)\n\n        # Add LoRA adapter\n        lora_config = LoraConfig(\n            r=8,\n            lora_alpha=32,\n            target_modules=[\"c_attn\"],\n            lora_dropout=0.1,\n        )\n        self.model = get_peft_model(self.model, lora_config)\n\n    def train(self, data_path: str, output_dir: str):\n        dataset = load_dataset(\"json\", data_files=data_path)\n\n        training_args = TrainingArguments(\n            output_dir=output_dir,\n            num_train_epochs=3,\n            per_device_train_batch_size=4,\n            save_steps=100,\n        )\n\n        trainer = Trainer(\n            model=self.model,\n            args=training_args,\n            train_dataset=dataset[\"train\"],\n        )\n\n        trainer.train()\n        trainer.save_model()\n</code></pre>"},{"location":"developer_onboarding/#step-2-export-to-onnx","title":"Step 2: Export to ONNX","text":"<pre><code>from transformers import AutoModelForCausalLM\nimport torch\n\nmodel = AutoModelForCausalLM.from_pretrained(\"output_dir\")\ndummy_input = torch.randint(0, 1000, (1, 128))\n\ntorch.onnx.export(\n    model,\n    dummy_input,\n    \"model.onnx\",\n    input_names=[\"input_ids\"],\n    output_names=[\"logits\"],\n    dynamic_axes={\"input_ids\": {0: \"batch\", 1: \"seq\"}}\n)\n</code></pre>"},{"location":"developer_onboarding/#step-3-update-microservice","title":"Step 3: Update Microservice","text":"<p>Modify the generated <code>app.py</code> to load ONNX:</p> <pre><code>import onnxruntime as ort\n\nsession = ort.InferenceSession(\"model.onnx\")\n\n@app.post(\"/predict\")\ndef predict(request: PredictRequest):\n    inputs = tokenizer(request.input, return_tensors=\"np\")\n    outputs = session.run(None, dict(inputs))\n    return {\"output\": decode(outputs), \"confidence\": 0.95}\n</code></pre>"},{"location":"developer_onboarding/#7-releasing-a-new-version","title":"7. Releasing a New Version","text":""},{"location":"developer_onboarding/#automated-release","title":"Automated Release","text":"<pre><code># Run the release script\nbash releases/prepare_release.sh 0.2.0\n\n# Review changes, then commit\ngit add .\ngit commit -m \"chore(release): v0.2.0\"\n\n# Tag and push\ngit tag -a v0.2.0 -m \"Release v0.2.0\"\ngit push origin main --tags\n</code></pre>"},{"location":"developer_onboarding/#manual-steps","title":"Manual Steps","text":"<ol> <li>Update <code>CHANGELOG.md</code></li> <li>Bump version in <code>pyproject.toml</code></li> <li>Run full test suite: <code>pytest -v</code></li> <li>Build package: <code>python -m build</code></li> <li>Upload to PyPI: <code>twine upload dist/*</code></li> <li>Create GitHub release</li> </ol> <p>See docs/release.md for detailed instructions.</p>"},{"location":"developer_onboarding/#8-code-style","title":"8. Code Style","text":"<ul> <li>Formatting: Use <code>black</code> for Python formatting</li> <li>Linting: Use <code>flake8</code> for linting</li> <li>Type hints: Use type hints for all public functions</li> <li>Docstrings: Use Google-style docstrings</li> <li>Tests: Aim for &gt;80% coverage</li> </ul> <pre><code># Format code\nblack cli backend connectors inference_server tests\n\n# Lint\nflake8 cli backend connectors inference_server\n\n# Type check (optional)\nmypy cli backend connectors inference_server\n</code></pre>"},{"location":"developer_onboarding/#9-common-tasks","title":"9. Common Tasks","text":""},{"location":"developer_onboarding/#add-a-cli-command","title":"Add a CLI Command","text":"<p>Edit <code>cli/foremforge.py</code>:</p> <pre><code>@cli.command()\n@click.option(\"--option\", help=\"Description\")\ndef my_command(option):\n    \"\"\"Command description.\"\"\"\n    click.echo(f\"Running with {option}\")\n</code></pre>"},{"location":"developer_onboarding/#add-an-api-endpoint","title":"Add an API Endpoint","text":"<p>Edit <code>inference_server/server.py</code>:</p> <pre><code>@app.get(\"/my-endpoint\")\ndef my_endpoint():\n    return {\"status\": \"ok\"}\n</code></pre>"},{"location":"developer_onboarding/#add-a-test","title":"Add a Test","text":"<pre><code># tests/test_my_feature.py\n\nimport pytest\n\nclass TestMyFeature:\n    def test_basic(self):\n        assert True\n\n    @pytest.fixture\n    def setup(self):\n        # Setup code\n        yield\n        # Teardown code\n</code></pre>"},{"location":"developer_onboarding/#10-getting-help","title":"10. Getting Help","text":"<ul> <li>Documentation: Check <code>docs/</code> directory</li> <li>Issues: GitHub Issues</li> <li>Contributing: See CONTRIBUTING.md</li> </ul> <p>Happy coding! Welcome to the TinyForgeAI community.</p>"},{"location":"launch/","title":"Launch Checklist","text":"<p>This document outlines the steps for launching TinyForgeAI publicly, including social media and community announcements.</p>"},{"location":"launch/#pre-launch-checklist","title":"Pre-Launch Checklist","text":""},{"location":"launch/#code-documentation","title":"Code &amp; Documentation","text":"<ul> <li>[ ] All tests passing</li> <li>[ ] README.md polished and up-to-date</li> <li>[ ] Documentation complete (architecture, training, connectors)</li> <li>[ ] Example files and demo scripts working</li> <li>[ ] LICENSE file present (Apache 2.0)</li> <li>[ ] CONTRIBUTING.md ready for contributors</li> </ul>"},{"location":"launch/#repository","title":"Repository","text":"<ul> <li>[ ] Repository is public</li> <li>[ ] GitHub topics/tags set (python, machine-learning, fine-tuning, fastapi)</li> <li>[ ] Description and website URL set</li> <li>[ ] Issue templates configured</li> <li>[ ] Branch protection on main</li> </ul>"},{"location":"launch/#package","title":"Package","text":"<ul> <li>[ ] PyPI package published</li> <li>[ ] Installation tested: <code>pip install tinyforgeai</code></li> <li>[ ] CLI commands working post-install</li> </ul>"},{"location":"launch/#launch-day","title":"Launch Day","text":""},{"location":"launch/#github-release","title":"GitHub Release","text":"<ol> <li>Create release with comprehensive notes</li> <li>Attach built artifacts (wheel, sdist)</li> <li>Link to documentation</li> </ol>"},{"location":"launch/#social-media","title":"Social Media","text":""},{"location":"launch/#twitterx","title":"Twitter/X","text":"<pre><code>\ud83d\ude80 Introducing TinyForgeAI - a lightweight platform for fine-tuning language models!\n\n\u2728 Train models from JSONL datasets\n\ud83d\udce6 Export to production-ready FastAPI services\n\ud83d\udc33 Docker support included\n\ud83d\udd0c Multiple data connectors (files, DBs, Google Docs)\n\nTry it: pip install tinyforgeai\n\nGitHub: [link]\n</code></pre>"},{"location":"launch/#linkedin","title":"LinkedIn","text":"<pre><code>Excited to share TinyForgeAI - an open-source platform that simplifies the journey from raw data to deployed ML models.\n\nKey features:\n\u2022 Complete fine-tuning pipeline with minimal configuration\n\u2022 One-click export to FastAPI inference services\n\u2022 Support for multiple data sources (files, databases, Google Docs)\n\u2022 Docker-ready deployment\n\nPerfect for:\n\u2022 ML engineers prototyping new models\n\u2022 Teams needing quick model deployment\n\u2022 Developers learning fine-tuning workflows\n\nCheck it out: [GitHub link]\n\n#MachineLearning #OpenSource #Python #AI\n</code></pre>"},{"location":"launch/#reddit","title":"Reddit","text":"<p>Post to relevant subreddits: - r/MachineLearning (Show and Tell) - r/Python - r/learnmachinelearning - r/LocalLLaMA (if applicable)</p> <pre><code>Title: [P] TinyForgeAI - Lightweight Fine-Tuning and Deployment Platform\n\nI've been working on TinyForgeAI, an open-source tool that simplifies\nfine-tuning language models and deploying them as APIs.\n\n**What it does:**\n- Train models from JSONL datasets\n- Export to production-ready FastAPI services\n- Docker support out of the box\n- Multiple data connectors (local files, databases, Google Docs)\n\n**Quick example:**\n```bash\npip install tinyforgeai\nforemforge train --data data.jsonl --out ./model\nforemforge export --model ./model/model_stub.json --out ./service\nforemforge serve --dir ./service --port 8000\n</code></pre> <p>GitHub: [link] Docs: [link]</p> <p>Feedback welcome! <pre><code>#### Hacker News\n</code></pre> Title: Show HN: TinyForgeAI \u2013 Lightweight Fine-Tuning and Model Deployment</p> <p>Body: TinyForgeAI is an open-source platform for fine-tuning language models and deploying them as inference services.</p> <p>It handles the entire pipeline: data ingestion \u2192 training \u2192 export \u2192 deployment, with minimal configuration required.</p> <p>Key features: - JSONL dataset format - FastAPI-based inference servers - Docker support - Multiple data connectors</p> <p>GitHub: [link] <pre><code>### Developer Communities\n\n#### Discord Servers\n- Python Discord\n- ML/AI focused servers\n- FastAPI community\n\n#### Slack Workspaces\n- MLOps Community\n- Python developers\n\n## Post-Launch\n\n### Week 1\n- [ ] Monitor GitHub issues\n- [ ] Respond to questions promptly\n- [ ] Track stars/forks growth\n- [ ] Note common questions for FAQ\n\n### Month 1\n- [ ] Write blog post about architecture\n- [ ] Create tutorial video (optional)\n- [ ] Gather user feedback\n- [ ] Plan next release based on feedback\n\n### Ongoing\n- [ ] Regular releases (monthly or as needed)\n- [ ] Community engagement\n- [ ] Documentation updates\n- [ ] Issue triage\n\n## Metrics to Track\n\n- GitHub stars\n- PyPI downloads\n- GitHub issues/PRs\n- Community mentions\n- Documentation page views\n\n## Announcement Templates\n\n### Short (Twitter-style)\n</code></pre> \ud83c\udf89 TinyForgeAI v0.1.0 is out! Fine-tune language models and deploy them as APIs in minutes. pip install tinyforgeai | GitHub: [link] <pre><code>### Medium (Newsletter)\n</code></pre> TinyForgeAI - From Raw Data to Deployed Models</p> <p>We're launching TinyForgeAI, a Python toolkit that streamlines the ML workflow from data preparation to production deployment.</p> <p>Get started in 3 commands: 1. foremforge train --data your_data.jsonl --out ./model 2. foremforge export --model ./model/model_stub.json --out ./service 3. foremforge serve --dir ./service --port 8000</p> <p>Learn more: [link] ```</p>"},{"location":"launch/#resources-to-prepare","title":"Resources to Prepare","text":"<ul> <li>[ ] Logo/banner image (1200x630 for social sharing)</li> <li>[ ] Architecture diagram</li> <li>[ ] Demo GIF showing CLI in action</li> <li>[ ] Comparison table (if applicable)</li> </ul>"},{"location":"launch/#contact-points","title":"Contact Points","text":"<p>For press or partnership inquiries: - GitHub Issues: [link] - Email: [project email if applicable]</p> <p>Good luck with the launch! \ud83d\ude80</p>"},{"location":"release/","title":"Release Guide","text":"<p>This guide covers how to publish a new release of TinyForgeAI.</p>"},{"location":"release/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10+</li> <li><code>build</code> package: <code>pip install build</code></li> <li><code>twine</code> package: <code>pip install twine</code></li> <li>PyPI account with upload permissions (if publishing to PyPI)</li> <li>GitHub CLI (<code>gh</code>) for creating releases</li> </ul>"},{"location":"release/#release-process","title":"Release Process","text":""},{"location":"release/#1-prepare-the-release","title":"1. Prepare the Release","text":"<p>Use the automated release preparation script:</p> <pre><code>./releases/prepare_release.sh 0.1.0\n</code></pre> <p>This script will: - \u2705 Verify working directory is clean - \u2705 Run the test suite - \u2705 Run linting (critical errors) - \u2705 Update version in <code>pyproject.toml</code> - \u2705 Update version in <code>tinyforgeai/__init__.py</code> - \u2705 Build sdist and wheel packages - \u2705 Verify packages with twine</p>"},{"location":"release/#2-update-changelog","title":"2. Update CHANGELOG","text":"<p>Edit <code>CHANGELOG.md</code> to: 1. Move items from <code>[Unreleased]</code> to the new version section 2. Add the release date 3. Create a fresh <code>[Unreleased]</code> section</p> <pre><code>## [Unreleased]\n\n## [0.1.0] - 2025-01-15\n\n### Added\n- Initial release features...\n</code></pre>"},{"location":"release/#3-commit-and-tag","title":"3. Commit and Tag","text":"<pre><code># Stage version changes\ngit add pyproject.toml tinyforgeai/__init__.py CHANGELOG.md\n\n# Commit\ngit commit -m \"chore: bump version to 0.1.0\"\n\n# Create annotated tag\ngit tag -a v0.1.0 -m \"Release v0.1.0\"\n\n# Push to remote\ngit push origin main\ngit push origin v0.1.0\n</code></pre>"},{"location":"release/#4-build-distribution","title":"4. Build Distribution","text":"<p>If not already built by the prep script:</p> <pre><code># Clean previous builds\nrm -rf dist/ build/ *.egg-info/\n\n# Build sdist and wheel\npython -m build\n\n# Verify\ntwine check dist/*\n</code></pre>"},{"location":"release/#5-upload-to-pypi","title":"5. Upload to PyPI","text":""},{"location":"release/#test-pypi-recommended-first","title":"Test PyPI (Recommended First)","text":"<pre><code>twine upload --repository testpypi dist/*\n</code></pre> <p>Test installation: <pre><code>pip install --index-url https://test.pypi.org/simple/ tinyforgeai\n</code></pre></p>"},{"location":"release/#production-pypi","title":"Production PyPI","text":"<pre><code>twine upload dist/*\n</code></pre>"},{"location":"release/#6-create-github-release","title":"6. Create GitHub Release","text":"<p>Using GitHub CLI:</p> <pre><code>gh release create v0.1.0 dist/* \\\n  --title \"v0.1.0\" \\\n  --notes-file releases/notes_initial_release.md\n</code></pre> <p>Or manually: 1. Go to https://github.com/anthropics/TinyForgeAI/releases/new 2. Choose the tag <code>v0.1.0</code> 3. Set title: \"v0.1.0\" 4. Copy content from <code>releases/notes_initial_release.md</code> 5. Upload files from <code>dist/</code> 6. Publish release</p>"},{"location":"release/#version-numbering","title":"Version Numbering","text":"<p>TinyForgeAI follows Semantic Versioning:</p> <ul> <li>MAJOR (1.0.0): Breaking API changes</li> <li>MINOR (0.1.0): New features, backward compatible</li> <li>PATCH (0.0.1): Bug fixes, backward compatible</li> </ul>"},{"location":"release/#pre-release-versions","title":"Pre-release Versions","text":"<p>For alpha/beta releases: - <code>0.1.0a1</code> - Alpha 1 - <code>0.1.0b1</code> - Beta 1 - <code>0.1.0rc1</code> - Release Candidate 1</p>"},{"location":"release/#checklist","title":"Checklist","text":"<p>Before releasing:</p> <ul> <li>[ ] All tests pass (<code>pytest -q</code>)</li> <li>[ ] No critical linting errors</li> <li>[ ] CHANGELOG.md updated</li> <li>[ ] Version bumped in all locations</li> <li>[ ] Documentation updated</li> <li>[ ] Release notes written</li> </ul> <p>After releasing:</p> <ul> <li>[ ] GitHub release created</li> <li>[ ] PyPI package uploaded</li> <li>[ ] Installation tested from PyPI</li> <li>[ ] Announcement posted (if applicable)</li> </ul>"},{"location":"release/#troubleshooting","title":"Troubleshooting","text":""},{"location":"release/#build-fails","title":"Build Fails","text":"<pre><code># Ensure build is installed\npip install --upgrade build\n\n# Try verbose build\npython -m build --verbose\n</code></pre>"},{"location":"release/#twine-upload-fails","title":"Twine Upload Fails","text":"<pre><code># Check credentials\ncat ~/.pypirc\n\n# Or use environment variables\nexport TWINE_USERNAME=__token__\nexport TWINE_PASSWORD=pypi-xxx...\n</code></pre>"},{"location":"release/#tag-already-exists","title":"Tag Already Exists","text":"<pre><code># Delete local tag\ngit tag -d v0.1.0\n\n# Delete remote tag (use with caution!)\ngit push --delete origin v0.1.0\n</code></pre>"},{"location":"release/#hotfix-releases","title":"Hotfix Releases","text":"<p>For urgent fixes:</p> <ol> <li> <p>Create hotfix branch from tag:    <pre><code>git checkout -b hotfix/0.1.1 v0.1.0\n</code></pre></p> </li> <li> <p>Apply fix and test</p> </li> <li> <p>Follow normal release process with patch version bump</p> </li> </ol>"},{"location":"release/#see-also","title":"See Also","text":"<ul> <li>RELEASE_CHECKLIST.md - Quick reference checklist</li> <li>CONTRIBUTING.md - Contribution guidelines</li> <li>releases/ - Release scripts and notes</li> </ul>"},{"location":"social_launch_kit/","title":"TinyForgeAI \u2014 Social Launch Kit","text":"<p>Ready-to-use content for launching TinyForgeAI across social platforms.</p>"},{"location":"social_launch_kit/#linkedin-post","title":"LinkedIn Post","text":"<pre><code>We just released TinyForgeAI \u2014 an open-source framework to train tiny language models from your own data and deploy them instantly as microservices.\n\nWhy this matters:\nMost companies don't need giant LLMs. They need small, private, deterministic AI engines trained only on their documents.\n\nTinyForgeAI gives you exactly that.\n\n\ud83d\udc49 Train a tiny model\n\ud83d\udc49 Export as a REST microservice\n\ud83d\udc49 Deploy anywhere (Docker/K8s)\n\ud83d\udc49 No ML expertise needed\n\nGitHub: https://github.com/anthropics/TinyForgeAI\n\nTry the demo: bash examples/e2e_demo.sh\n\n#OpenSource #MachineLearning #Python #AI #MLOps #TinyML\n</code></pre>"},{"location":"social_launch_kit/#xtwitter-post","title":"X/Twitter Post","text":"<pre><code>\ud83d\udd25 Just launched TinyForgeAI \u2014 train tiny language models from your own data \u2192 deploy as microservices in seconds.\n\nSimple CLI. Dry-run training. Connectors (DB, Docs, Files). ONNX export. Docker. E2E demo.\n\nCheck it out \u2192 https://github.com/anthropics/TinyForgeAI\n\n#AI #ML #Python #OpenSource\n</code></pre>"},{"location":"social_launch_kit/#thread-version","title":"Thread Version","text":"<pre><code>\ud83e\uddf5 1/5\nIntroducing TinyForgeAI \u2014 the simplest way to go from raw data to deployed ML model.\n\nNo ML expertise required. Just your data + 3 commands.\n</code></pre> <pre><code>2/5\nThe problem: LLMs are overkill for most business use cases.\n\nYou don't need 70B parameters to answer questions about YOUR documents.\n\nYou need a small, fast, private model trained on YOUR data.\n</code></pre> <pre><code>3/5\nTinyForgeAI does exactly that:\n\n\u2705 Ingest data (DB, files, Google Docs)\n\u2705 Train a tiny model (dry-run or real)\n\u2705 Export as microservice\n\u2705 Deploy with Docker\n\nAll from the CLI.\n</code></pre> <pre><code>4/5\nQuick demo:\n\nforemforge train --data data.jsonl --out ./model\nforemforge export --model ./model/model_stub.json --out ./service\nforemforge serve --dir ./service --port 8000\n\nThat's it. You now have a REST API.\n</code></pre> <pre><code>5/5\nTinyForgeAI is open source (Apache 2.0).\n\nGitHub: https://github.com/anthropics/TinyForgeAI\n\nStar it \u2b50 if you find it useful!\n\n#AI #MachineLearning #Python #OpenSource\n</code></pre>"},{"location":"social_launch_kit/#reddit-post","title":"Reddit Post","text":""},{"location":"social_launch_kit/#rmachinelearning-show-and-tell","title":"r/MachineLearning (Show and Tell)","text":"<p>Title: [P] TinyForgeAI - Lightweight Fine-Tuning and Deployment Platform</p> <pre><code>I've been working on TinyForgeAI, an open-source tool that simplifies fine-tuning language models and deploying them as APIs.\n\n**What it does:**\n- Train models from JSONL datasets\n- Export to production-ready FastAPI services\n- Docker support out of the box\n- Multiple data connectors (local files, databases, Google Docs)\n\n**Quick example:**\n\npip install tinyforgeai\nforemforge train --data data.jsonl --out ./model --dry-run\nforemforge export --model ./model/model_stub.json --out ./service\nforemforge serve --dir ./service --port 8000\n\n**Why I built it:**\nMost ML deployment tools assume you already have a trained model. TinyForgeAI handles the entire pipeline from raw data to deployed service.\n\n**Current status:**\n- v0.1.0 released\n- Dry-run training (real training coming soon)\n- 215+ tests passing\n- Full documentation\n\nGitHub: https://github.com/anthropics/TinyForgeAI\nDocs: https://github.com/anthropics/TinyForgeAI/tree/main/docs\n\nFeedback welcome! Especially interested in:\n- What connectors you'd want (APIs, cloud storage, etc.)\n- Export formats beyond ONNX\n- UI preferences\n\nLicense: Apache 2.0\n</code></pre>"},{"location":"social_launch_kit/#rpython","title":"r/Python","text":"<p>Title: TinyForgeAI: Train and deploy ML models with Python + FastAPI</p> <pre><code>Just released TinyForgeAI - a Python framework for the complete ML workflow.\n\n**Stack:**\n- Click for CLI\n- FastAPI for inference servers\n- Pydantic for data validation\n- Docker for deployment\n\n**Features:**\n- JSONL dataset handling\n- Multiple data connectors (files, DB, Google Docs)\n- Auto-generated microservices\n- Health checks, metrics, Docker support\n\n**Install:**\npip install tinyforgeai\n\n**Try it:**\npython examples/e2e_demo.py\n\nGitHub: https://github.com/anthropics/TinyForgeAI\n\nBuilt with Python 3.10+, fully typed, 215+ tests.\n</code></pre>"},{"location":"social_launch_kit/#rlocalllama","title":"r/LocalLLaMA","text":"<p>Title: TinyForgeAI - Tool for training and deploying small, local LLMs</p> <pre><code>For those running local models, I built TinyForgeAI to simplify the training \u2192 deployment pipeline.\n\n**Key features:**\n- Train tiny models from your own data\n- Export to ONNX (coming: quantization)\n- Generate complete FastAPI services\n- Docker-ready deployment\n- No cloud required\n\nThe idea: not every use case needs GPT-4. Sometimes a small model trained on your specific data is better (faster, cheaper, private).\n\nCurrently uses a stub trainer for prototyping. Real HuggingFace integration is on the roadmap.\n\nGitHub: https://github.com/anthropics/TinyForgeAI\n</code></pre>"},{"location":"social_launch_kit/#hacker-news","title":"Hacker News","text":"<p>Title: Show HN: TinyForgeAI \u2013 Train tiny LLMs from your data, deploy as microservices</p> <pre><code>TinyForgeAI is an open-source platform for fine-tuning small language models and deploying them as inference services.\n\nThe thesis: Large LLMs are overkill for most enterprise use cases. A small model trained only on your data is often faster, cheaper, and more private.\n\nKey features:\n- JSONL dataset format\n- Multiple data connectors (files, databases, Google Docs)\n- FastAPI-based inference servers\n- Docker support\n- Full CLI (foremforge)\n\nQuick start:\n\n    foremforge train --data data.jsonl --out ./model --dry-run\n    foremforge export --model ./model/model_stub.json --out ./service\n    foremforge serve --dir ./service --port 8000\n\nCurrent version (v0.1.0) uses a stub trainer for rapid prototyping. Real fine-tuning (HuggingFace Transformers + LoRA) is the next priority.\n\nGitHub: https://github.com/anthropics/TinyForgeAI\nDocs: https://github.com/anthropics/TinyForgeAI/tree/main/docs\n\nLooking for feedback on:\n1. What data connectors would be most useful?\n2. What export formats matter to you?\n3. Would a web UI be valuable?\n\nApache 2.0 licensed.\n</code></pre>"},{"location":"social_launch_kit/#product-hunt","title":"Product Hunt","text":""},{"location":"social_launch_kit/#tagline-options","title":"Tagline Options","text":"<ol> <li>\"Train tiny AI models from your data, deploy as microservices\"</li> <li>\"From raw data to deployed ML API in 3 commands\"</li> <li>\"The simplest way to train and deploy your own language model\"</li> </ol>"},{"location":"social_launch_kit/#first-comment","title":"First Comment","text":"<pre><code>Thanks for checking out TinyForgeAI!\n\nWe built it because small LLMs are the future \u2014 private, cheap, fast, and fine-tuned to your business.\n\nThis v0.1.0 release includes:\n\u2705 Full CLI for training and export\n\u2705 Multiple data connectors\n\u2705 Auto-generated FastAPI microservices\n\u2705 Docker deployment\n\u2705 Complete documentation\n\nWhat's next:\n- Real fine-tuning (HuggingFace + LoRA)\n- More export formats\n- Web dashboard\n\nWe'd love your feedback! What features would make this more useful for you?\n</code></pre>"},{"location":"social_launch_kit/#description","title":"Description","text":"<pre><code>TinyForgeAI is an open-source framework that lets you train small language models from your own data and deploy them as REST microservices \u2014 all from the command line.\n\nPerfect for:\n\ud83c\udfe2 Companies wanting private, on-premise AI\n\ud83d\udc69\u200d\ud83d\udcbb Developers who don't want to learn complex ML pipelines\n\ud83d\udd12 Teams needing secure, data-local inference\n\u26a1 Edge deployment scenarios\n\nFeatures:\n\u2022 Simple CLI: train, export, serve\n\u2022 Multiple data sources: files, databases, Google Docs\n\u2022 Auto-generated FastAPI services\n\u2022 Docker-ready deployment\n\u2022 Extensible connector system\n\nNo ML expertise required. Apache 2.0 licensed.\n</code></pre>"},{"location":"social_launch_kit/#taglines-short","title":"Taglines (Short)","text":"<ul> <li>\"Tiny models. Big impact.\"</li> <li>\"Your data. Your model. Your microservice.\"</li> <li>\"Train \u2192 Export \u2192 Serve. Done.\"</li> <li>\"ML deployment without the PhD.\"</li> <li>\"Small AI for real problems.\"</li> </ul>"},{"location":"social_launch_kit/#newsletterblog-intro","title":"Newsletter/Blog Intro","text":"<pre><code>Introducing TinyForgeAI \u2014 From Raw Data to Deployed Models\n\nWe're launching TinyForgeAI, a Python toolkit that streamlines the ML workflow from data preparation to production deployment.\n\nThe problem with large language models:\n- Expensive to run\n- Overkill for specific tasks\n- Privacy concerns with cloud APIs\n- Complex to fine-tune\n\nTinyForgeAI's approach:\n- Train small models on YOUR data\n- Deploy as simple REST services\n- Run anywhere (local, cloud, edge)\n- No ML expertise required\n\nGet started in 3 commands:\n\n1. foremforge train --data your_data.jsonl --out ./model\n2. foremforge export --model ./model/model_stub.json --out ./service\n3. foremforge serve --dir ./service --port 8000\n\nGitHub: https://github.com/anthropics/TinyForgeAI\n</code></pre>"},{"location":"social_launch_kit/#imagegraphics-suggestions","title":"Image/Graphics Suggestions","text":"<p>For social posts, consider creating:</p> <ol> <li>Architecture diagram (already in README)</li> <li>CLI demo GIF \u2014 Show the 3 commands running</li> <li>Before/After comparison \u2014 \"Traditional ML pipeline\" vs \"TinyForgeAI\"</li> <li>Logo/Banner \u2014 1200x630 for social sharing</li> </ol>"},{"location":"social_launch_kit/#hashtags","title":"Hashtags","text":"<pre><code>#TinyForgeAI #OpenSource #MachineLearning #Python #AI #MLOps #FastAPI #Docker #TinyML #EdgeAI #LocalAI #LLM #FineTuning #DataScience\n</code></pre>"},{"location":"training/","title":"TinyForgeAI Training Documentation","text":"<p>Last updated: 2025-01-15T00:00:00Z</p> <p>This document describes the training capabilities of TinyForgeAI, including dataset format, dry-run training, PEFT/LoRA integration, and model artifacts.</p>"},{"location":"training/#dataset-format","title":"Dataset Format","text":"<p>TinyForgeAI uses JSONL (JSON Lines) format for training data. Each line must be a valid JSON object.</p>"},{"location":"training/#required-fields","title":"Required Fields","text":"Field Type Description <code>input</code> string The input text/prompt for the model <code>output</code> string The expected output/response"},{"location":"training/#optional-fields","title":"Optional Fields","text":"Field Type Description <code>metadata</code> object Additional information (source, category, etc.)"},{"location":"training/#example-jsonl-file","title":"Example JSONL File","text":"<pre><code>{\"input\": \"How do I reset my password?\", \"output\": \"Go to Settings &gt; Reset Password.\"}\n{\"input\": \"What is your refund policy?\", \"output\": \"Refunds within 30 days with receipt.\"}\n{\"input\": \"Where can I find my order history?\", \"output\": \"Check your Account &gt; Orders section.\"}\n</code></pre>"},{"location":"training/#sample-data","title":"Sample Data","text":"<p>A sample dataset is provided at <code>examples/sample_qna.jsonl</code>:</p> <pre><code># View the sample data\ncat examples/sample_qna.jsonl\n</code></pre> <pre><code>{\"input\": \"How do I reset my password?\", \"output\": \"Go to Settings &gt; Account &gt; Reset Password and follow the instructions.\"}\n{\"input\": \"What is your refund policy?\", \"output\": \"We offer full refunds within 30 days of purchase with a valid receipt.\"}\n{\"input\": \"How can I contact support?\", \"output\": \"You can reach our support team at support@example.com or call 1-800-SUPPORT.\"}\n</code></pre>"},{"location":"training/#loading-data-programmatically","title":"Loading Data Programmatically","text":"<pre><code>from backend.training.dataset import load_jsonl, stream_jsonl, summarize_dataset\n\n# Load all records into memory\nrecords = load_jsonl(\"examples/sample_qna.jsonl\")\nprint(f\"Loaded {len(records)} records\")\n\n# Stream records for large files (memory efficient)\nfor record in stream_jsonl(\"examples/sample_qna.jsonl\"):\n    print(record[\"input\"][:50])\n\n# Get dataset statistics\nsummary = summarize_dataset(records)\nprint(f\"Records: {summary['n_records']}\")\nprint(f\"Avg input length: {summary['avg_input_len']:.2f} tokens\")\nprint(f\"Avg output length: {summary['avg_output_len']:.2f} tokens\")\n</code></pre>"},{"location":"training/#dry-run-training","title":"Dry-Run Training","text":"<p>The dry-run trainer validates your dataset and creates a stub model artifact without performing actual training. This is useful for:</p> <ul> <li>Validating dataset format and content</li> <li>Testing the training pipeline end-to-end</li> <li>Generating placeholder artifacts for export testing</li> </ul>"},{"location":"training/#running-dry-run-training","title":"Running Dry-Run Training","text":"<pre><code># Basic dry-run\npython backend/training/train.py --data examples/sample_qna.jsonl --out /tmp/tiny_model --dry-run\n</code></pre>"},{"location":"training/#cli-flags","title":"CLI Flags","text":"Flag Description <code>--data</code> Path to JSONL training data file (required) <code>--out</code> Output directory for model artifacts (required) <code>--dry-run</code> Validate data and create stub artifact only <code>--use-lora</code> Apply LoRA adapter to the model"},{"location":"training/#using-the-foremforge-cli","title":"Using the foremforge CLI","text":"<pre><code># Dry-run training with CLI\nforemforge train --data examples/sample_qna.jsonl --out /tmp/tiny_model --dry-run\n\n# With LoRA adapter\nforemforge train --data examples/sample_qna.jsonl --out /tmp/tiny_model --dry-run --use-lora\n</code></pre>"},{"location":"training/#output","title":"Output","text":"<p>The trainer produces:</p> <pre><code>/tmp/tiny_model/\n\u2514\u2500\u2500 model_stub.json    # Model metadata and configuration\n</code></pre> <p>Example <code>model_stub.json</code>:</p> <pre><code>{\n  \"model_type\": \"tinyforge_stub\",\n  \"n_records\": 3,\n  \"created_time\": \"2025-01-15T12:00:00+00:00\",\n  \"notes\": \"dry-run artifact\"\n}\n</code></pre>"},{"location":"training/#peft-lora-hook","title":"PEFT / LoRA Hook","text":""},{"location":"training/#what-is-peftlora","title":"What is PEFT/LoRA?","text":"<p>PEFT (Parameter-Efficient Fine-Tuning) is a family of techniques for fine-tuning large language models using fewer trainable parameters. LoRA (Low-Rank Adaptation) is a popular PEFT method that injects trainable low-rank matrices into transformer layers, enabling efficient fine-tuning while preserving most of the original model weights.</p>"},{"location":"training/#tinyforgeai-lora-stub","title":"TinyForgeAI LoRA Stub","text":"<p>The <code>apply_lora(model, config)</code> function simulates LoRA adapter application during dry-run training. This allows you to test your training pipeline with LoRA configuration before integrating a real PEFT implementation.</p>"},{"location":"training/#usage","title":"Usage","text":"<p>CLI with LoRA:</p> <pre><code>python backend/training/train.py --data examples/sample_qna.jsonl --out /tmp/tiny_model --use-lora --dry-run\n</code></pre> <p>Programmatic usage:</p> <pre><code>from backend.training.peft_adapter import apply_lora\n\nmodel = {\"model_type\": \"tinyforge_stub\", \"n_records\": 3}\npatched = apply_lora(model)\n\n# patched now contains:\n# - lora_applied: True\n# - lora_config: {\"r\": 8, \"alpha\": 16, \"target_modules\": [\"q\", \"v\"]}\n# - lora_timestamp: ISO8601 UTC timestamp\n</code></pre>"},{"location":"training/#default-lora-configuration","title":"Default LoRA Configuration","text":"Parameter Default Description <code>r</code> 8 LoRA rank (low-rank dimension) <code>alpha</code> 16 Scaling factor <code>target_modules</code> <code>[\"q\", \"v\"]</code> Transformer modules to apply LoRA to"},{"location":"training/#output-with-lora","title":"Output with LoRA","text":"<p>When <code>--use-lora</code> is enabled, the <code>model_stub.json</code> includes LoRA metadata:</p> <pre><code>{\n  \"model_type\": \"tinyforge_stub\",\n  \"n_records\": 3,\n  \"created_time\": \"2025-01-15T12:00:00+00:00\",\n  \"notes\": \"dry-run artifact\",\n  \"lora_applied\": true,\n  \"lora_config\": {\n    \"r\": 8,\n    \"alpha\": 16,\n    \"target_modules\": [\"q\", \"v\"]\n  },\n  \"lora_timestamp\": \"2025-01-15T12:00:00+00:00\"\n}\n</code></pre>"},{"location":"training/#how-to-replace-stub-with-real-training","title":"How to Replace Stub with Real Training","text":"<p>To integrate real model training with Hugging Face Transformers and PEFT:</p>"},{"location":"training/#checklist","title":"Checklist","text":"<ol> <li>Pick a Base Model</li> <li>Choose a suitable model (e.g., <code>t5-small</code>, <code>gpt2</code>, <code>llama-2-7b</code>)</li> <li> <p>Consider model size vs. available compute resources</p> </li> <li> <p>Prepare Tokenization and Dataset Mapping <pre><code>from transformers import AutoTokenizer\nfrom datasets import Dataset\n\ntokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n\ndef tokenize_function(examples):\n    inputs = tokenizer(examples[\"input\"], truncation=True, max_length=512)\n    targets = tokenizer(examples[\"output\"], truncation=True, max_length=128)\n    inputs[\"labels\"] = targets[\"input_ids\"]\n    return inputs\n</code></pre></p> </li> <li> <p>Use PEFT (LoRA) or Full Fine-Tune <pre><code>from peft import LoraConfig, get_peft_model\n\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    target_modules=[\"q\", \"v\"],\n    lora_dropout=0.1,\n)\nmodel = get_peft_model(base_model, lora_config)\n</code></pre></p> </li> <li> <p>Convert Adapters/Weights into Deployable Artifact <pre><code># Save LoRA adapters\nmodel.save_pretrained(\"./lora_adapters\")\n\n# For deployment, merge weights\nmerged_model = model.merge_and_unload()\nmerged_model.save_pretrained(\"./merged_model\")\n</code></pre></p> </li> <li> <p>Add Test/Validation Holdout and Monitoring</p> </li> <li>Split data: 80% train, 10% validation, 10% test</li> <li>Track metrics: loss, perplexity, task-specific metrics</li> <li>Use early stopping based on validation performance</li> </ol>"},{"location":"training/#artifacts-exports","title":"Artifacts &amp; Exports","text":""},{"location":"training/#model-stub-model_stubjson","title":"Model Stub (model_stub.json)","text":"<p>Created by dry-run trainer:</p> <pre><code>{\n  \"model_type\": \"tinyforge_stub\",\n  \"n_records\": 3,\n  \"created_time\": \"2025-01-15T12:00:00+00:00\",\n  \"notes\": \"dry-run artifact\"\n}\n</code></pre>"},{"location":"training/#onnx-placeholder-modelonnx","title":"ONNX Placeholder (model.onnx)","text":"<p>Created by <code>--export-onnx</code> flag:</p> <pre><code># TinyForgeAI ONNX placeholder\n# Exported: 2025-01-15T12:00:00+00:00\n# Source: /tmp/tiny_model/model_stub.json\n# This is a stub file for testing purposes.\n</code></pre> <p>With metadata in <code>model.onnx.meta.json</code>:</p> <pre><code>{\n  \"exported_from\": \"/tmp/tiny_model/model_stub.json\",\n  \"export_time\": \"2025-01-15T12:00:00+00:00\",\n  \"version\": \"0.1-stub\",\n  \"source_model\": {\n    \"model_type\": \"tinyforge_stub\",\n    \"n_records\": 3\n  }\n}\n</code></pre>"},{"location":"training/#quantized-placeholder-quantizedonnx","title":"Quantized Placeholder (quantized.onnx)","text":"<p>Created by quantization hook:</p> <pre><code>{\n  \"exported_from\": \"/tmp/tiny_model/model_stub.json\",\n  \"export_time\": \"2025-01-15T12:00:00+00:00\",\n  \"version\": \"0.1-stub\",\n  \"quantized\": true,\n  \"mode\": \"int8\",\n  \"quantize_time\": \"2025-01-15T12:00:01+00:00\"\n}\n</code></pre>"},{"location":"training/#model-metadata-model_metadatajson","title":"Model Metadata (model_metadata.json)","text":"<p>Created by the exporter builder in the microservice output:</p> <pre><code>{\n  \"model_path\": \"/tmp/tiny_model/model_stub.json\",\n  \"created_time\": \"2025-01-15T12:00:00+00:00\",\n  \"source\": \"tinyforge-exporter\",\n  \"model_stub\": true\n}\n</code></pre>"},{"location":"training/#export-report-export_reportjson","title":"Export Report (export_report.json)","text":"<p>Created when <code>--export-onnx</code> is used:</p> <pre><code>{\n  \"onnx_path\": \"/tmp/tiny_service/onnx/model.onnx\",\n  \"quantized_path\": \"/tmp/tiny_service/onnx/quant/quantized.onnx\",\n  \"export_time\": \"2025-01-15T12:00:00+00:00\",\n  \"model_path\": \"/tmp/tiny_model/model_stub.json\"\n}\n</code></pre>"},{"location":"training/#complete-example-workflow","title":"Complete Example Workflow","text":"<pre><code># 1. Train (dry-run)\npython backend/training/train.py \\\n  --data examples/sample_qna.jsonl \\\n  --out /tmp/tiny_model \\\n  --dry-run \\\n  --use-lora\n\n# 2. Export with ONNX\npython backend/exporter/builder.py \\\n  --model-path /tmp/tiny_model/model_stub.json \\\n  --output-dir /tmp/tiny_service \\\n  --overwrite \\\n  --export-onnx\n\n# 3. Inspect artifacts\nls -la /tmp/tiny_model/\ncat /tmp/tiny_model/model_stub.json\n\nls -la /tmp/tiny_service/\ncat /tmp/tiny_service/model_metadata.json\n\nls -la /tmp/tiny_service/onnx/\ncat /tmp/tiny_service/export_report.json\n</code></pre>"},{"location":"training/#related-documentation","title":"Related Documentation","text":"<ul> <li>Architecture Overview - System design and components</li> <li>Data Connectors - Loading data from various sources</li> <li>CI/CD - Continuous integration setup</li> </ul>"}]}